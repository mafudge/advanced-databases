{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64433da7-6a56-4e3e-94f8-a62a7110a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-53104aa5-2b13-47fb-84ad-b28b851154aa;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.1.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.1.2/hadoop-aws-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.1.2!hadoop-aws.jar (150ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.271/aws-java-sdk-bundle-1.11.271.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.271!aws-java-sdk-bundle.jar (63722ms)\n",
      ":: resolution report :: resolve 1710ms :: artifacts dl 63876ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.1.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-53104aa5-2b13-47fb-84ad-b28b851154aa\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (84646kB/44ms)\n",
      "23/02/22 17:29:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "bucket = \"labe\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.1.2\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa317006-5295-47f7-9bcc-8c6fbaec6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EST: string (nullable = true)\n",
      " |-- Max TemperatureF: integer (nullable = true)\n",
      " |-- Mean TemperatureF: integer (nullable = true)\n",
      " |-- Min TemperatureF: integer (nullable = true)\n",
      " |-- Max Dew PointF: integer (nullable = true)\n",
      " |-- MeanDew PointF: integer (nullable = true)\n",
      " |-- Min DewpointF: integer (nullable = true)\n",
      " |-- Max Humidity: integer (nullable = true)\n",
      " |-- Mean Humidity: integer (nullable = true)\n",
      " |-- Min Humidity: integer (nullable = true)\n",
      " |-- Max Sea Level PressureIn: double (nullable = true)\n",
      " |-- Mean Sea Level PressureIn: double (nullable = true)\n",
      " |-- Min Sea Level PressureIn: double (nullable = true)\n",
      " |-- Max VisibilityMiles: integer (nullable = true)\n",
      " |-- Mean VisibilityMiles: integer (nullable = true)\n",
      " |-- Min VisibilityMiles: integer (nullable = true)\n",
      " |-- Max Wind SpeedMPH: integer (nullable = true)\n",
      " |-- Mean Wind SpeedMPH: integer (nullable = true)\n",
      " |-- Max Gust SpeedMPH: integer (nullable = true)\n",
      " |-- PrecipitationIn: string (nullable = true)\n",
      " |-- CloudCover: integer (nullable = true)\n",
      " |-- Events: string (nullable = true)\n",
      " |-- WindDirDegrees: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather = spark.read\\\n",
    "    .option(\"header\",True).option(\"inferSchema\",True)\\\n",
    "    .csv(\"s3a://labe/syracuse-ny.csv\")\n",
    "\n",
    "weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e756107c-a165-43a5-b3a8-6c4227ecfd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EST: string (nullable = true)\n",
      " |-- Max TemperatureF: integer (nullable = true)\n",
      " |-- Mean TemperatureF: integer (nullable = true)\n",
      " |-- Min TemperatureF: integer (nullable = true)\n",
      " |-- Max Dew PointF: integer (nullable = true)\n",
      " |-- MeanDew PointF: integer (nullable = true)\n",
      " |-- Min DewpointF: integer (nullable = true)\n",
      " |-- Max Humidity: integer (nullable = true)\n",
      " |-- Mean Humidity: integer (nullable = true)\n",
      " |-- Min Humidity: integer (nullable = true)\n",
      " |-- Max Sea Level PressureIn: double (nullable = true)\n",
      " |-- Mean Sea Level PressureIn: double (nullable = true)\n",
      " |-- Min Sea Level PressureIn: double (nullable = true)\n",
      " |-- Max VisibilityMiles: integer (nullable = true)\n",
      " |-- Mean VisibilityMiles: integer (nullable = true)\n",
      " |-- Min VisibilityMiles: integer (nullable = true)\n",
      " |-- Max Wind SpeedMPH: integer (nullable = true)\n",
      " |-- Mean Wind SpeedMPH: integer (nullable = true)\n",
      " |-- Max Gust SpeedMPH: integer (nullable = true)\n",
      " |-- PrecipitationIn: string (nullable = true)\n",
      " |-- CloudCover: integer (nullable = true)\n",
      " |-- Events: string (nullable = true)\n",
      " |-- WindDirDegrees: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "    .option(\"header\",True).option(\"inferSchema\",True)\\\n",
    "    .csv(\"s3a://labe/syracuse-ny.csv\")\\\n",
    "    .createOrReplaceTempView(\"weather\")\n",
    "\n",
    "spark.sql(\"select * from weather\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90fd812f-f34a-4fdc-952c-cc1bd9e05baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [year#191 ASC NULLS FIRST, month#192 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#191 ASC NULLS FIRST, month#192 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#97]\n",
      "   +- *(2) HashAggregate(keys=[year#191, month#192], functions=[avg(cast(mintemp#193 as bigint)), avg(cast(maxtemp#194 as bigint))])\n",
      "      +- Exchange hashpartitioning(year#191, month#192, 200), ENSURE_REQUIREMENTS, [id=#93]\n",
      "         +- *(1) HashAggregate(keys=[year#191, month#192], functions=[partial_avg(cast(mintemp#193 as bigint)), partial_avg(cast(maxtemp#194 as bigint))])\n",
      "            +- *(1) Project [cast(split(EST#16, -, -1)[0] as int) AS year#191, cast(split(EST#16, -, -1)[1] as int) AS month#192, Min TemperatureF#19 AS mintemp#193, Max TemperatureF#17 AS maxtemp#194]\n",
      "               +- FileScan csv [EST#16,Max TemperatureF#17,Min TemperatureF#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[s3a://labe/syracuse-ny.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<EST:string,Max TemperatureF:int,Min TemperatureF:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "with source as  (\n",
    "    select \n",
    "        cast(split(EST,'-')[0] as int) as year,\n",
    "        cast(split(EST,'-')[1] as int) as month,\n",
    "        `Min TemperatureF` as mintemp,\n",
    "        `Max TemperatureF` as maxtemp\n",
    "    from weather\n",
    ")\n",
    "select \n",
    "    year, month, avg(mintemp) as avgmin, avg(maxtemp) as avgmax\n",
    "    from source \n",
    "    group by year, month\n",
    "    order by year, month\n",
    "\n",
    "'''\n",
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699bd41b-db1c-4b10-929c-e0209d43e34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|        |monthly_syracuse_...|       true|\n",
      "|        |             weather|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "with source as  (\n",
    "    select \n",
    "        cast(split(EST,'-')[0] as int) as year,\n",
    "        cast(split(EST,'-')[1] as int) as month,\n",
    "        `Min TemperatureF` as mintemp,\n",
    "        `Max TemperatureF` as maxtemp\n",
    "    from weather\n",
    ")\n",
    "select \n",
    "    year, month, avg(mintemp) as avgmin, avg(maxtemp) as avgmax\n",
    "    from source \n",
    "    group by year, month\n",
    "    order by year, month\n",
    "\n",
    "'''\n",
    "spark.sql(query).createOrReplaceTempView(\"monthly_syracuse_weather_averages\")\n",
    "\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decec622-eca2-42b1-ae28-685f565978d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Month:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+-----------------+\n",
      "|year|month|            avgmin|           avgmax|\n",
      "+----+-----+------------------+-----------------+\n",
      "|1997|    7| 59.87096774193548|80.19354838709677|\n",
      "|1998|    7| 61.29032258064516|79.03225806451613|\n",
      "|1999|    7|  64.3225806451613|85.74193548387096|\n",
      "|2000|    7|57.774193548387096|76.51612903225806|\n",
      "|2001|    7|              59.0|79.87096774193549|\n",
      "|2002|    7| 63.61290322580645|84.16129032258064|\n",
      "|2003|    7|61.483870967741936|81.29032258064517|\n",
      "|2004|    7|60.903225806451616|78.16129032258064|\n",
      "|2005|    7| 64.19354838709677|85.12903225806451|\n",
      "|2006|    7|              65.0|83.12903225806451|\n",
      "|2007|    7|59.483870967741936|80.19354838709677|\n",
      "|2008|    7| 61.41935483870968|81.19354838709677|\n",
      "|2009|    7|58.935483870967744|77.45161290322581|\n",
      "|2010|    7| 64.38709677419355|84.64516129032258|\n",
      "|2011|    7| 64.64516129032258|86.93548387096774|\n",
      "|2012|    7| 64.54838709677419|87.90322580645162|\n",
      "|2013|    7| 65.06451612903226|83.83870967741936|\n",
      "|2014|    7| 61.32258064516129|81.48387096774194|\n",
      "|2015|    7|60.806451612903224|81.16129032258064|\n",
      "+----+-----+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "month = input(\"Enter Month: \")\n",
    "spark.sql(f\"select * from monthly_syracuse_weather_averages where month = {month}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3255cacb-4d4a-47f2-9a1f-8e2c38d77cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|max(year)|max(month)|\n",
      "+---------+----------+\n",
      "|     2015|        12|\n",
      "+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select max(year),max(month) from monthly_syracuse_weather_averages\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e547241-9ad2-4c56-b6f2-f5acec2e2ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<H1>Pick a Month</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828acc76818a4609afe512c3a27d3757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='Month', max=12, min=1), Dropdown(description='Year', opt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact_manual\n",
    "\n",
    "display(HTML(\"<H1>Pick a Month</h1>\") )\n",
    "@interact_manual(Month=(1,12), Year=[2020, 2021, 2022])\n",
    "def onclick(Month,Year):\n",
    "    display(HTML(f\"You selected: <b>{Month}</b> <em>{Year}</em>\"))\n",
    "    \n",
    "# type = tuple = slider of int/floats\n",
    "# type = string = textbox\n",
    "# type = list = dropdown\n",
    "# type = boolean = checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00aac6e-ed40-4239-a61d-6304434368b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d0049805b743e997c11d3b5d47e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='name'), Checkbox(value=False, description='is_grad'), Checkb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(name=\"\", is_grad=False, is_su=False, gpa=(0.0,4.0) )\n",
    "def onclick(name, is_grad, is_su, gpa):\n",
    "    print(\"do your analysis\")\n",
    "    print(name, is_grad, is_su, gpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c969604d-384a-4921-b483-8379bc1f3e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<H1>Syracuse Weather</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b251d71d4ed416e8b716c780868c120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='Month', max=12, min=1), Button(description='Run Interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display(HTML(\"<H1>Syracuse Weather</h1>\") )\n",
    "@interact_manual(Month=(1,12))\n",
    "def doit(Month):\n",
    "    df = spark.sql(f\"select * from monthly_syracuse_weather_averages where month = {Month}\").toPandas()\n",
    "    display(df)\n",
    "    df.set_index(\"year\")\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(df[\"year\"],y=df[\"avgmin\"], label='monthly avg min', marker='v')\n",
    "    plt.scatter(df[\"year\"],y=df[\"avgmax\"], label='monthly avg max', marker='^')\n",
    "    plt.legend(title=f\"Temps for Month {Month}\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e022c45b-de64-4617-a744-51cc1886fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = spark.read.json(\"s3a://labe/reddit.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "123196a5-ddd6-48a3-8ee7-31f8eed6d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            children|\n",
      "+--------------------+\n",
      "|[{{null, false, h...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit.select(\"data.children\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097cf8c6-d7aa-4627-b183-29736785a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9072c357-edef-48d9-84d4-3d55dff7d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = spark.read.json(\"s3a://labe/reddit.json\")\n",
    "\n",
    "x =reddit.select( explode( \"data.children\") )\n",
    "\n",
    "y = x.select(\"col.data\").select(\"data.id\",\"data.name\",\"data.title\",\"data.author\",\"data.score\",\"data.url\")\n",
    "\n",
    "y.createOrReplaceTempView(\"reddit\")\n",
    "\n",
    "spark.sql(\"select * from reddit\").write.mode(\"overwrite\").csv(\"s3a://labe/reddit.csv\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97013b10-7fbe-408f-b4aa-5e58fb597ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f47633-bb02-4477-a659-7f0c02aff749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
