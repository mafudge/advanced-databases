{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980b0a7e-32a3-4eb5-ba57-22a15400f334",
   "metadata": {},
   "source": [
    "# Unit D\n",
    "# Object Storage and PySpark Programming\n",
    "\n",
    "- Examples From Video Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cca4c16-c9d9-4c97-ba2c-4ca858fd063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "bucket = \"d-object-spark\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-avro_2.12:3.1.2\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac9b2d3-4d45-4751-9f55-64cc6e399c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context :  <SparkContext master=local appName=jupyter-pyspark>\n",
      "Spark Version :  3.5.3\n",
      "Spark appName : jupyter-pyspark\n",
      "Hadoop version:  3.3.4\n",
      "Spark Confiuration:\n",
      "\tspark.master = local\n",
      "\tspark.app.initial.jar.urls = spark://jupyter:42647/jars/org.spark-project.spark_unused-1.0.0.jar,spark://jupyter:42647/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,spark://jupyter:42647/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,spark://jupyter:42647/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,spark://jupyter:42647/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar\n",
      "\tspark.hadoop.fs.s3a.path.style.access = true\n",
      "\tspark.hadoop.fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n",
      "\tspark.files = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\thive.metastore.uris = thrift://hive-metastore:9083\n",
      "\tspark.app.name = jupyter-pyspark\n",
      "\tspark.driver.port = 42647\n",
      "\tspark.jars.packages = org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-avro_2.12:3.1.2\n",
      "\tspark.serializer.objectStreamReset = 100\n",
      "\tspark.app.startTime = 1734396562584\n",
      "\tspark.submit.deployMode = client\n",
      "\tspark.submit.pyFiles = /home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,/home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,/home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,/home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "\tspark.hadoop.fs.s3a.access.key = minio\n",
      "\tspark.driver.host = jupyter\n",
      "\tspark.app.id = local-1734396563384\n",
      "\tspark.hadoop.fs.s3a.secret.key = SU2orange!\n",
      "\tspark.executor.id = driver\n",
      "\tspark.repl.local.jars = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.jars = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\tspark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "\tspark.sql.catalogImplementation = hive\n",
      "\tspark.rdd.compress = True\n",
      "\tspark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "\tspark.app.submitTime = 1734396562327\n",
      "\tspark.hadoop.fs.s3a.endpoint = http://minio:9000\n",
      "\tspark.ui.showConsoleProgress = true\n",
      "\tspark.hadoop.fs.s3a.fast.upload = true\n",
      "\tspark.app.initial.file.urls = file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.1.2.jar\n"
     ]
    }
   ],
   "source": [
    "# Print context\n",
    "print('Spark Context : ', spark.sparkContext)\n",
    "print('Spark Version : ', spark.sparkContext.version)\n",
    "print('Spark appName :', spark.sparkContext.appName)\n",
    "print('Hadoop version: ', spark.sparkContext._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n",
    "print('Spark Confiuration:')\n",
    "for conf in spark.sparkContext._conf.getAll():\n",
    "    print(f\"\\t{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea47639-14dd-412d-847b-058269deea61",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Put data in the right places!!!\n",
    "- Run these cells to ensure you have the data for the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be171b5-a42b-4464-b30b-a5b67f3fd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /home/jovyan/.local/lib/python3.11/site-packages (7.2.12)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2024.8.30)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from minio) (2.2.3)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from minio) (23.1.0)\n",
      "Requirement already satisfied: pycryptodome in /home/jovyan/.local/lib/python3.11/site-packages (from minio) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from minio) (4.12.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n"
     ]
    }
   ],
   "source": [
    "! pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e9d556-5cf5-4e68-b624-c1717a3f4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "|1725.05|  GOOG|\n",
      "| 128.39|   IBM|\n",
      "| 212.55|  MSFT|\n",
      "|   78.0|   NET|\n",
      "|  497.0|  NFLX|\n",
      "|  823.8|  TSLA|\n",
      "|  45.11|  TWTR|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "\n",
    "# Make the minio bucket\n",
    "client = Minio(\"minio:9000\",\"minio\",\"SU2orange!\", secure=False)\n",
    "not client.bucket_exists(bucket) and client.make_bucket(bucket)\n",
    "\n",
    "# open the example \n",
    "df = spark.read.csv(\"/home/jovyan/datasets/stocks/stocks.csv\", header=True)\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a1bbdf-76ae-4582-b109-fcba9ee8a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the example in minio\n",
    "df.write.mode(\"Overwrite\").csv(f\"s3a://{bucket}/stocks.csv\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7db8e8-0d0a-49a2-af6e-9cc0f0a3199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the example in HDFS\n",
    "df.write.mode(\"Overwrite\").csv(f\"hdfs://namenode/user/root/{bucket}/stocks.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e77e52-f221-4cf2-b306-4e6e537b8cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Minio Client\n",
    "\n",
    "This section outlines commands from the minio client\n",
    "\n",
    "These commands are run from the terminal in your jupyter notebook setup\n",
    "\n",
    "### Minio Alias Setup\n",
    "\n",
    "```\n",
    "Installing the client\n",
    "\n",
    "$ wget https://dl.min.io/client/mc/release/linux-amd64/mc && chmod +x mc && sudo mv -f  mc /usr/local/bin\n",
    "\n",
    "# view aliases\n",
    "\n",
    "mc alias list\n",
    "\n",
    "# create alias to our server, which we will call \"ms\"\n",
    "\n",
    "mc alias set ms http://minio:9000 minio SU2orange!\n",
    "\n",
    "# to delete an alias its\n",
    "\n",
    "ms alias rm ms\n",
    "\n",
    "```\n",
    "\n",
    "### Minio File and bucket commands\n",
    "\n",
    "These are similar to the `hadoop fs` commands. \n",
    "\n",
    "```\n",
    "#make bucket testing \n",
    "mc mb play/testing\n",
    "\n",
    "# list buckets on the play alias\n",
    "mc ls play\n",
    "\n",
    "# copy files to the play/testing bucket\n",
    "\n",
    "mc cp /datasets/customers/* play/testing\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18e09e-103c-4554-9feb-02140d4f844e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab31cb43-5d49-4f8d-84fa-b5b76c0b9045",
   "metadata": {},
   "source": [
    "## Reading Data into the Spark Dataframe: Paths\n",
    "\n",
    "Spark can read (and write) data from a variety of locations, just by including the proper path to the file.\n",
    "\n",
    "- `file://` read a file off the local file system. Not ideal for a clustered environment. Use `SparkFiles`.\n",
    "- `s3a://` read from our object storage configration\n",
    "- `hdfs://` head from hadoop's HDFS using the client\n",
    "- `webhdfs://` head from hadoop's HDFS using the web client\n",
    "- `https://` read over the web - must use `SparkFiles`. See Next Section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dee5b48-1b0d-4f16-8ac1-1e8ebff16d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file://\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "s3a://\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "hdfs://\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "webhdfs://\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"file://\") # Not Ideal!\n",
    "spark.read.csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\", header=True).show(3)\n",
    "\n",
    "print(\"s3a://\")\n",
    "spark.read.csv(f\"s3a://{bucket}/stocks.csv\", header=True).show(3)\n",
    "\n",
    "print(\"hdfs://\")\n",
    "spark.read.csv(f\"hdfs://namenode/user/root/{bucket}/stocks.csv/\", header=True).show(3)\n",
    "\n",
    "print(\"webhdfs://\")\n",
    "spark.read.csv(f\"webhdfs://namenode:50070/user/root/{bucket}/stocks.csv/\", header=True).show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e998b-0229-4768-b1c7-eea6447fb70d",
   "metadata": {},
   "source": [
    "## Reading Data : `SparkFiles`\n",
    "\n",
    "Let's not forget Spark is a distributed computing environment. Reading a local file, or file off the web into our cluster doesn't help spark take advantage of its distributed nature. So to do that we need to use `SparkFiles` which registers the file with the `sparkContext` of the `sparkSession`. This, in essence makes the cluster aware of the file.\n",
    "\n",
    "`spark.sparkContext.addFile(url)` will  download the file at `url` and add it to the tmp location on the worker nodes in the cluster.\n",
    "\n",
    "When you need the file, use `SparkFiles.get(filename)` to retrieve its path.\n",
    "\n",
    "NOTES: \n",
    "\n",
    "- You add a file by path, but access the file by name. \n",
    "- You cannot add the same file name more than once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a172cb-dd6d-4c90-83da-81ff8473a4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary Location:  /tmp/spark-df1421bc-01c5-4706-8412-e7f75c95f1fe/userFiles-7b48d9d2-fba3-4b6f-bb72-46108c08dba0/stocks.json\n",
      "https://\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                   [|\n",
      "|  { \"symbol\" : \"A...|\n",
      "|  { \"symbol\" : \"A...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/mafudge/datasets/refs/heads/master/ist256/09-Dictionaries/stocks.json\")\n",
    "\n",
    "print(\"Temporary Location: \", SparkFiles.get(\"stocks.json\"))\n",
    "\n",
    "print(\"https://\")\n",
    "spark.read.text(SparkFiles.get(\"stocks.json\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc094b-cc0f-4dfb-a178-7aa7f8a5c105",
   "metadata": {},
   "source": [
    "## Reading Data : Wildcards\n",
    "\n",
    "You don't have to read a single file. Instead you can read an entire folder of files, or a wildcard match of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf38abed-8b10-4bbf-a072-9df3a46753fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read just fall\n",
      "+----+----+------+---+---+\n",
      "| _c0| _c1|   _c2|_c3|_c4|\n",
      "+----+----+------+---+---+\n",
      "|2016|Fall|IST346|  3|  A|\n",
      "|2016|Fall|CHE111|  4| A-|\n",
      "|2016|Fall|PSY120|  3| B+|\n",
      "|2016|Fall|IST256|  3|  A|\n",
      "|2016|Fall|ENG121|  3| B+|\n",
      "|2015|Fall|IST101|  1|  A|\n",
      "|2015|Fall|IST195|  3|  A|\n",
      "|2015|Fall|IST233|  3| B+|\n",
      "|2015|Fall|SOC101|  3| A-|\n",
      "|2015|Fall|MAT221|  3|  C|\n",
      "+----+----+------+---+---+\n",
      "\n",
      "read all files\n",
      "+----+------+------+---+---+\n",
      "| _c0|   _c1|   _c2|_c3|_c4|\n",
      "+----+------+------+---+---+\n",
      "|2016|  Fall|IST346|  3|  A|\n",
      "|2016|  Fall|CHE111|  4| A-|\n",
      "|2016|  Fall|PSY120|  3| B+|\n",
      "|2016|  Fall|IST256|  3|  A|\n",
      "|2016|  Fall|ENG121|  3| B+|\n",
      "|2015|  Fall|IST101|  1|  A|\n",
      "|2015|  Fall|IST195|  3|  A|\n",
      "|2015|  Fall|IST233|  3| B+|\n",
      "|2015|  Fall|SOC101|  3| A-|\n",
      "|2015|  Fall|MAT221|  3|  C|\n",
      "|2016|Spring|GEO110|  3| B+|\n",
      "|2016|Spring|MAT222|  3|  A|\n",
      "|2016|Spring|SOC121|  3| C+|\n",
      "|2016|Spring|BIO240|  3| B-|\n",
      "|2017|Spring|IST462|  3|  A|\n",
      "|2017|Spring|MAT411|  3|  C|\n",
      "|2017|Spring|SOC422|  3| B-|\n",
      "|2017|Spring|ENV201|  3| A-|\n",
      "+----+------+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"read just fall\")\n",
    "spark.read.csv(\"file:///home/jovyan/datasets/grades/fall*.tsv\", sep=\"\\t\").show()\n",
    "\n",
    "# read all of them\n",
    "print(\"read all files\")\n",
    "spark.read.csv(\"file:///home/jovyan/datasets/grades/\", sep=\"\\t\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32dadd-d08a-4b5c-8edd-a275226a6747",
   "metadata": {},
   "source": [
    "## Reading Data: File Formats\n",
    "\n",
    "Spark can read data in a variety of formats. Each format has configurable options.\n",
    "\n",
    "- `csv` delimited (comma, tab, etc) file\n",
    "- `text` generic text file, one row per line\n",
    "- `json` JSON format \n",
    "- `parquet` Parquet format (common big-data format with schema included)\n",
    "- `orc` Another common big-data format with schema.\n",
    "\n",
    "Each format has options to change behaviors of the file format. Use the `option()` method to set them.\n",
    "\n",
    "More Information: https://spark.apache.org/docs/latest/sql-data-sources.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3772252a-726e-4b27-9bdf-523b7c4e8b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "read Parquet file\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Read JSON file\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Read a pipe-separated file\n",
      "+-------------------+--------------------+--------------------+--------+--------------------+\n",
      "|                _c0|                 _c1|                 _c2|     _c3|                 _c4|\n",
      "+-------------------+--------------------+--------------------+--------+--------------------+\n",
      "|2845428583999282239|1.4337661612984276E9|Mon Jun 08 08:22:...|rovlight|Why so horrible d...|\n",
      "|1658183905022391067|1.4298210344679017E9|Thu Apr 23 16:30:...|   sladd|Just placed an or...|\n",
      "| 973476786498736360|1.4421079524352274E9|Sat Sep 12 21:32:...| rdeboat|Worst purchase ev...|\n",
      "+-------------------+--------------------+--------------------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle headers\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").show(3)\n",
    "\n",
    "# Infer schema from the columns\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").show(3)\n",
    "\n",
    "# readng a schema based file has less options\n",
    "print(\"read Parquet file\")\n",
    "spark.read \\\n",
    "    .parquet(\"file:///home/jovyan/datasets/stocks/stocks.parquet\").show(3)\n",
    "\n",
    "\n",
    "# JSON file format - there are many options for this file format\n",
    "print(\"Read JSON file\")\n",
    "spark.read.option(\"multiline\",True).json(\"/home/jovyan/datasets/json-samples/stocks.json\").show(3)\n",
    "\n",
    "# This is not comma-delimited\n",
    "print(\"Read a pipe-separated file\")\n",
    "spark.read \\\n",
    "    .option(\"sep\",\"|\") \\\n",
    "    .option(\"header\",False) \\\n",
    "    .option(\"inferSchema\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/tweets/tweets.psv\").show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21038d68-ff56-4cad-ba8c-78a7aac98bd9",
   "metadata": {},
   "source": [
    "## Caching DataFrames\n",
    "\n",
    "The `cache()` function will persist the `DataFrame` to temp storage on the spark cluster. This can be in-memory, on disk, or both depending on the cluster size and data set size.\n",
    "\n",
    "This is specially useful when the data source is external to the spark cluster (a remote database, for example) and it will be retrieved and transformed multiple times.\n",
    "\n",
    "`cache()` forces lazy evaluation so any transformation prior to caching are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea9e3130-5f15-4cc2-a28f-64cc655437e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://\n",
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "| 126.82|  AAPL|\n",
      "|3098.12|  AMZN|\n",
      "| 251.11|    FB|\n",
      "+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"s3a://\")\n",
    "stocks = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(f\"s3a://{bucket}/stocks.csv\").cache()\n",
    "stocks.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cf2fb-60e5-468f-ac19-158ffc9ced25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataFrame Schemas\n",
    "\n",
    "Every spark dataframe has a schema, or collection of typed columns. The schema is stored in a `StructType` and the columns are `StructFields` consisting of the field name and a specific `StructType`\n",
    "\n",
    "- When you `spark.read` data, from  the schema is always the most flexible type, `StringType`.\n",
    "- When you include the `inferSchema` option, and extra pass is made over the data to infer the `StructType` for each column.\n",
    "- For formats that include a schema, like `parquet` or `orc` the schema in the file is loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4884abb-1993-401c-9307-99c3895d809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks: No Schema\n",
      "root\n",
      " |-- price: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "Stocks: Infer Schema\n",
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "Customers...\n",
      "root\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Last IP Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Total Orders: integer (nullable = true)\n",
      " |-- Total Purchased: integer (nullable = true)\n",
      " |-- Months Customer: integer (nullable = true)\n",
      "\n",
      "+-----+------+--------------------+------+---------------+--------+-----+------------+---------------+---------------+\n",
      "|First|  Last|               Email|Gender|Last IP Address|    City|State|Total Orders|Total Purchased|Months Customer|\n",
      "+-----+------+--------------------+------+---------------+--------+-----+------------+---------------+---------------+\n",
      "|   Al|Fresco|  afresco@dayrep.com|     M|  74.111.18.161|Syracuse|   NY|           1|             45|              1|\n",
      "| Abby|  Kuss|     akuss@rhyta.com|     F|  23.80.125.101| Phoenix|   AZ|           1|             25|              2|\n",
      "|Arial| Photo|   aphoto@dayrep.com|     F|     24.0.14.56|  Newark|   NJ|           1|            680|              1|\n",
      "|Bette| Alott|    balott@rhyta.com|     F| 56.216.127.219| Raleigh|   NC|           6|            560|             18|\n",
      "|Barb |Barion|bbarion@superrito...|     F|   38.68.15.223|  Dallas|   TX|           4|           1590|              1|\n",
      "+-----+------+--------------------+------+---------------+--------+-----+------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Stocks: No Schema\")\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").printSchema()\n",
    "\n",
    "# Infer schema from the columns\n",
    "print(\"Stocks: Infer Schema\")\n",
    "spark.read \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/stocks/stocks.csv\").printSchema()\n",
    "\n",
    "\n",
    "# This is not comma-delimited\n",
    "print(\"Customers...\")\n",
    "customers = spark.read \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\",True) \\\n",
    "    .csv(\"file:///home/jovyan/datasets/customers/customers.csv\")\n",
    "    \n",
    "customers.printSchema()\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c79cda-1fe5-4407-930c-04f4d95b40bc",
   "metadata": {},
   "source": [
    "## DataFrame Schemas: Nested Schema\n",
    "\n",
    "Spark handles file formats with nested schemas, such as `json` very well. This means you can read from Document and Graph databases easily. \n",
    "\n",
    "- Embedded columns can be additional `StructType` columns or `ArrayType` for nested lists of values.\n",
    "- Later we will introduce strategies for dealing with nested schema like this one|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06cc2be1-77f4-416c-8399-aa91bc6f6258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers...\n",
      "root\n",
      " |-- business_status: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- location: struct (nullable = true)\n",
      " |    |    |-- lat: double (nullable = true)\n",
      " |    |    |-- lng: double (nullable = true)\n",
      " |    |-- viewport: struct (nullable = true)\n",
      " |    |    |-- northeast: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- lng: double (nullable = true)\n",
      " |    |    |-- southwest: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- lng: double (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- icon_background_color: string (nullable = true)\n",
      " |-- icon_mask_base_uri: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- opening_hours: struct (nullable = true)\n",
      " |    |-- open_now: boolean (nullable = true)\n",
      " |-- photos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html_attributions: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- photo_reference: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- plus_code: struct (nullable = true)\n",
      " |    |-- compound_code: string (nullable = true)\n",
      " |    |-- global_code: string (nullable = true)\n",
      " |-- price_level: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- reference: string (nullable = true)\n",
      " |-- scope: string (nullable = true)\n",
      " |-- types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_ratings_total: long (nullable = true)\n",
      " |-- vicinity: string (nullable = true)\n",
      "\n",
      "+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+-----------+------+--------------------+------+--------------------+------------------+--------------------+\n",
      "|business_status|            geometry|                icon|icon_background_color|  icon_mask_base_uri|                name|opening_hours|              photos|            place_id|           plus_code|price_level|rating|           reference| scope|               types|user_ratings_total|            vicinity|\n",
      "+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+-----------+------+--------------------+------+--------------------+------------------+--------------------+\n",
      "|           NULL|{{43.0481221, -76...|https://maps.gsta...|              #7B9EB0|https://maps.gsta...|            Syracuse|         NULL|[{1080, [<a href=...|ChIJDZqXv5vz2YkRR...|                NULL|       NULL|  NULL|ChIJDZqXv5vz2YkRR...|GOOGLE|[locality, politi...|              NULL|            Syracuse|\n",
      "|    OPERATIONAL|{{43.0476078, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|Crowne Plaza Syra...|       {true}|[{2048, [<a href=...|ChIJXxPu66Tz2YkRr...|{2VX5+27 Syracuse...|       NULL|   4.1|ChIJXxPu66Tz2YkRr...|GOOGLE|[lodging, point_o...|              1153|701 East Genesee ...|\n",
      "|    OPERATIONAL|{{43.0476157, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|  The Parkview Hotel|       {true}|[{2584, [<a href=...|ChIJrWsN9KTz2YkRF...|{2VX5+2J Syracuse...|       NULL|   4.3|ChIJrWsN9KTz2YkRF...|GOOGLE|[lodging, point_o...|               350|713 East Genesee ...|\n",
      "|    OPERATIONAL|{{43.0472894, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|Jefferson Clinton...|       {true}|[{4096, [<a href=...|ChIJa_hOyrjz2YkRP...|{2RWW+WF Syracuse...|       NULL|   4.4|ChIJa_hOyrjz2YkRP...|GOOGLE|[lodging, point_o...|               397|416 South Clinton...|\n",
      "|    OPERATIONAL|{{43.0488846, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|Courtyard by Marr...|       {true}|[{1192, [<a href=...|ChIJGzEmOsfz2YkRm...|{2RXV+HH Syracuse...|       NULL|   4.1|ChIJGzEmOsfz2YkRm...|GOOGLE|[lodging, point_o...|               396|300 West Fayette ...|\n",
      "+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+-----------+------+--------------------+------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is not comma-delimited\n",
    "print(\"Customers...\")\n",
    "places = spark.read \\\n",
    "    .json(\"file:///home/jovyan/datasets/json-samples/google-places.json\")\n",
    "    \n",
    "places.printSchema()\n",
    "places.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13762e3f-6fce-4f06-9d11-933c028827f8",
   "metadata": {},
   "source": [
    "## Column Transformations\n",
    "\n",
    " - `withColumnRenamed()` – rename a column\n",
    " - `toDF()` – rename all columns\n",
    " - `withColumn()` – overwrite an existing column, deriving new columns\n",
    " - `drop()` – remove a column\n",
    " - `select()` - column projections\n",
    "\n",
    "\n",
    "### Setting Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849b45fc-e5e9-42be-a408-8826c1144ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Columns Names... yuck\n",
      "+----+----+------+---+---+\n",
      "| _c0| _c1|   _c2|_c3|_c4|\n",
      "+----+----+------+---+---+\n",
      "|2016|Fall|IST346|  3|  A|\n",
      "|2016|Fall|CHE111|  4| A-|\n",
      "|2016|Fall|PSY120|  3| B+|\n",
      "|2016|Fall|IST256|  3|  A|\n",
      "|2016|Fall|ENG121|  3| B+|\n",
      "+----+----+------+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Rename first two columns\n",
      "+----+--------+------+---+---+\n",
      "|Year|Semester|   _c2|_c3|_c4|\n",
      "+----+--------+------+---+---+\n",
      "|2016|    Fall|IST346|  3|  A|\n",
      "|2016|    Fall|CHE111|  4| A-|\n",
      "|2016|    Fall|PSY120|  3| B+|\n",
      "|2016|    Fall|IST256|  3|  A|\n",
      "|2016|    Fall|ENG121|  3| B+|\n",
      "+----+--------+------+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Rename all the columns\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "+----+--------+------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/*.tsv\")\n",
    "\n",
    "print(\"Default Columns Names... yuck\")\n",
    "grades.show(5)\n",
    "\n",
    "print(\"Rename first two columns\")\n",
    "grades2 = grades.withColumnRenamed(\"_c0\",\"Year\").withColumnRenamed(\"_c1\",\"Semester\")\n",
    "grades2.show(5)\n",
    "\n",
    "print(\"Rename all the columns\")\n",
    "grades3 = grades.toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "grades3.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabb49d-a127-4949-a6be-0f2609fa27d3",
   "metadata": {},
   "source": [
    "### Derived Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97242b6-1026-4795-82a4-161c3b27161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Semester: string (nullable = true)\n",
      " |-- Course: string (nullable = true)\n",
      " |-- Credits: integer (nullable = true)\n",
      " |-- Grade: string (nullable = true)\n",
      " |-- Next Year: integer (nullable = true)\n",
      " |-- YearString: string (nullable = true)\n",
      " |-- NullCol: void (nullable = true)\n",
      "\n",
      "+----+--------+------+-------+-----+---------+----------+-------+\n",
      "|Year|Semester|Course|Credits|Grade|Next Year|YearString|NullCol|\n",
      "+----+--------+------+-------+-----+---------+----------+-------+\n",
      "|2016|    Fall|IST346|      3|    A|     2017|      2016|   NULL|\n",
      "|2016|    Fall|CHE111|      4|   A-|     2017|      2016|   NULL|\n",
      "|2016|    Fall|PSY120|      3|   B+|     2017|      2016|   NULL|\n",
      "|2016|    Fall|IST256|      3|    A|     2017|      2016|   NULL|\n",
      "|2016|    Fall|ENG121|      3|   B+|     2017|      2016|   NULL|\n",
      "|2015|    Fall|IST101|      1|    A|     2016|      2015|   NULL|\n",
      "|2015|    Fall|IST195|      3|    A|     2016|      2015|   NULL|\n",
      "|2015|    Fall|IST233|      3|   B+|     2016|      2015|   NULL|\n",
      "|2015|    Fall|SOC101|      3|   A-|     2016|      2015|   NULL|\n",
      "|2015|    Fall|MAT221|      3|    C|     2016|      2015|   NULL|\n",
      "|2016|  Spring|GEO110|      3|   B+|     2017|      2016|   NULL|\n",
      "|2016|  Spring|MAT222|      3|    A|     2017|      2016|   NULL|\n",
      "|2016|  Spring|SOC121|      3|   C+|     2017|      2016|   NULL|\n",
      "|2016|  Spring|BIO240|      3|   B-|     2017|      2016|   NULL|\n",
      "|2017|  Spring|IST462|      3|    A|     2018|      2017|   NULL|\n",
      "|2017|  Spring|MAT411|      3|    C|     2018|      2017|   NULL|\n",
      "|2017|  Spring|SOC422|      3|   B-|     2018|      2017|   NULL|\n",
      "|2017|  Spring|ENV201|      3|   A-|     2018|      2017|   NULL|\n",
      "+----+--------+------+-------+-----+---------+----------+-------+\n",
      "\n",
      "+----+--------+------+-------+-----+---------+----------+\n",
      "|Year|Semester|Course|Credits|Grade|Next Year|YearString|\n",
      "+----+--------+------+-------+-----+---------+----------+\n",
      "|2016|    Fall|IST346|      3|    A|     2017|      2016|\n",
      "|2016|    Fall|CHE111|      4|   A-|     2017|      2016|\n",
      "|2016|    Fall|PSY120|      3|   B+|     2017|      2016|\n",
      "|2016|    Fall|IST256|      3|    A|     2017|      2016|\n",
      "|2016|    Fall|ENG121|      3|   B+|     2017|      2016|\n",
      "|2015|    Fall|IST101|      1|    A|     2016|      2015|\n",
      "|2015|    Fall|IST195|      3|    A|     2016|      2015|\n",
      "|2015|    Fall|IST233|      3|   B+|     2016|      2015|\n",
      "|2015|    Fall|SOC101|      3|   A-|     2016|      2015|\n",
      "|2015|    Fall|MAT221|      3|    C|     2016|      2015|\n",
      "|2016|  Spring|GEO110|      3|   B+|     2017|      2016|\n",
      "|2016|  Spring|MAT222|      3|    A|     2017|      2016|\n",
      "|2016|  Spring|SOC121|      3|   C+|     2017|      2016|\n",
      "|2016|  Spring|BIO240|      3|   B-|     2017|      2016|\n",
      "|2017|  Spring|IST462|      3|    A|     2018|      2017|\n",
      "|2017|  Spring|MAT411|      3|    C|     2018|      2017|\n",
      "|2017|  Spring|SOC422|      3|   B-|     2018|      2017|\n",
      "|2017|  Spring|ENV201|      3|   A-|     2018|      2017|\n",
      "+----+--------+------+-------+-----+---------+----------+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deriving a column\n",
    "from pyspark.sql.functions import lit\n",
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "\n",
    "grades2 = grades.withColumn(\"Next Year\",grades[\"Year\"] + 1) \\\n",
    "    .withColumn(\"YearString\", grades['Year'].cast(\"String\") ) \\\n",
    "    .withColumn(\"NullCol\", lit(None) )\n",
    "grades2.printSchema()\n",
    "grades2.show()\n",
    "\n",
    "grades2 = grades2.drop(\"NullCol\").show()\n",
    "grades3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f2269-d592-4c0b-b4f6-07e734748e14",
   "metadata": {},
   "source": [
    "### Column Projections with `select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16339eb7-2039-428f-a8a7-73fa50a9a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Course|Grade|\n",
      "+------+-----+\n",
      "|IST346|    A|\n",
      "|CHE111|   A-|\n",
      "|PSY120|   B+|\n",
      "|IST256|    A|\n",
      "|ENG121|   B+|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+\n",
      "|Course|Grade|\n",
      "+------+-----+\n",
      "|IST346|    A|\n",
      "|CHE111|   A-|\n",
      "|PSY120|   B+|\n",
      "|IST256|    A|\n",
      "|ENG121|   B+|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+\n",
      "|Course|Grade|\n",
      "+------+-----+\n",
      "|IST346|    A|\n",
      "|CHE111|   A-|\n",
      "|PSY120|   B+|\n",
      "|IST256|    A|\n",
      "|ENG121|   B+|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "# string references\n",
    "grades.select(\"Course\", \"Grade\").show(5)\n",
    "\n",
    "# Object property references\n",
    "grades.select(grades.Course, grades.Grade).show(5)\n",
    "\n",
    "# Dataframe references\n",
    "grades.select(grades[\"Course\"], grades[\"Grade\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ace579-dc7a-425b-9c14-1a754f02c5b5",
   "metadata": {},
   "source": [
    "## Row Transformations\n",
    "\n",
    "- `where()` or `filter()` apply a row based filter\n",
    "- `distinct()` remove duplicates\n",
    "- `sort()` or `orderBy()` sort by columns\n",
    "\n",
    "\n",
    "### Where / Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "562b5a33-1b31-42c4-8823-8c1cf38d902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A grades\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "print(\"A grades\")\n",
    "# string references\n",
    "grades.filter(\"Grade = 'A' or Grade='A-'\").show()\n",
    "\n",
    "# Object property references\n",
    "grades.filter( (grades.Grade == \"A\") | (grades.Grade == \"A-\") ).show()\n",
    "\n",
    "# Dataframe references\n",
    "grades.filter( (grades[\"Grade\"] == \"A\") | (grades[\"Grade\"] == \"A-\") ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37d6c8-b5b4-4c44-a1b9-f84face566d2",
   "metadata": {},
   "source": [
    "### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c30ca80-db47-4418-add8-2e240df61cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms\n",
      "+----+--------+\n",
      "|Year|Semester|\n",
      "+----+--------+\n",
      "|2016|    Fall|\n",
      "|2016|    Fall|\n",
      "|2016|    Fall|\n",
      "|2016|    Fall|\n",
      "|2016|    Fall|\n",
      "|2015|    Fall|\n",
      "|2015|    Fall|\n",
      "|2015|    Fall|\n",
      "|2015|    Fall|\n",
      "|2015|    Fall|\n",
      "|2016|  Spring|\n",
      "|2016|  Spring|\n",
      "|2016|  Spring|\n",
      "|2016|  Spring|\n",
      "|2017|  Spring|\n",
      "|2017|  Spring|\n",
      "|2017|  Spring|\n",
      "|2017|  Spring|\n",
      "+----+--------+\n",
      "\n",
      "Distinct Terms\n",
      "+----+--------+\n",
      "|Year|Semester|\n",
      "+----+--------+\n",
      "|2016|    Fall|\n",
      "|2017|  Spring|\n",
      "|2015|    Fall|\n",
      "|2016|  Spring|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = grades.select(\"Year\",\"Semester\")\n",
    "print(\"Terms\")\n",
    "terms.show()\n",
    "print(\"Distinct Terms\")\n",
    "dterms = terms.distinct()\n",
    "dterms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b6b12-b0b6-4696-831a-a842374aacf4",
   "metadata": {},
   "source": [
    "### Sort / orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2506588-2972-4dd7-8f6e-f30c629a954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "print(\"Sorting\")\n",
    "# string references\n",
    "grades.sort(\"Year\",\"Course\").show()\n",
    "\n",
    "# Object property references\n",
    "grades.sort(grades.Year, grades.Course.desc() ).show()\n",
    "\n",
    "# Dataframe references\n",
    "grades.sort( grades[\"Year\"], grades[\"Course\"].desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7a6c4-82e8-4cba-9c3d-428ed10394e4",
   "metadata": {},
   "source": [
    "## Aggregate Transformations\n",
    "\n",
    "- `groupBy()`  - perform a column grouping,similar to SQL group by,  returns a `GroupedData`\n",
    "- `agg()` - allows the application of an aggregate function to the `GroupedData`, returns a `DataFrame`\n",
    "- `alias()` - used to assign a name to a derived column\n",
    "- Aggregate Functions `count(), avg(), max(), min(), sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c25e7174-fa58-4c38-8d81-152ffcabe1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|TotalCredits|CourseCount|\n",
      "+------------+-----------+\n",
      "|          53|         18|\n",
      "+------------+-----------+\n",
      "\n",
      "+----+--------+-----------+------------+\n",
      "|Year|Semester|CourseCount|TotalCredits|\n",
      "+----+--------+-----------+------------+\n",
      "|2015|    Fall|          5|          13|\n",
      "|2016|  Spring|          4|          12|\n",
      "|2016|    Fall|          5|          16|\n",
      "|2017|  Spring|          4|          12|\n",
      "+----+--------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum,avg,max,min,count\n",
    "\n",
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "totalcredits = grades.groupBy().agg( sum(\"Credits\").alias(\"TotalCredits\"), count(\"*\").alias(\"CourseCount\") )\n",
    "totalcredits.show() \n",
    "\n",
    "termcredits = grades.groupBy(\"Year\", \"Semester\").agg( \\\n",
    "    count(\"*\").alias(\"CourseCount\"), sum(\"Credits\").alias(\"TotalCredits\") \\\n",
    "    ).sort(\"Year\",col(\"Semester\").desc())\n",
    "termcredits.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb453c9-637a-4e1a-b69b-c9d49eafef41",
   "metadata": {},
   "source": [
    "## Merge Transformations\n",
    "\n",
    "- `join()` -Merge data frame by column matching SQL join. Requires a join type string:\n",
    "    - \"inner\"  - SQL-like inner join\n",
    "    - \"full\" - SQL-like full outer join\n",
    "    - \"left\" - SQL-like left join\n",
    "    - \"right\" - SQL-Like right join\n",
    "    - \"cross\" - Cartesan Product \n",
    "- `union()` - merge two data frames by row, duplicates included, use `distinct()` to remove them.\n",
    "\n",
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a601eabd-3bf8-4419-8065-a2ca936118c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|_c0|  _c1|\n",
      "+---+-----+\n",
      "|  A|  4.0|\n",
      "| A-|3.666|\n",
      "| B+|3.333|\n",
      "|  B|  3.0|\n",
      "| B-|2.666|\n",
      "| C+|2.333|\n",
      "|  C|  2.0|\n",
      "| C-|1.666|\n",
      "|  D|  1.0|\n",
      "|  F|  0.0|\n",
      "+---+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradepoints = spark.read.option(\"inferSchema\",True).csv(\"file:///home/jovyan/datasets/courses/grade-points.csv\")\n",
    "gradepoints.show()\n",
    "grades.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ed02eea-33be-4381-b65e-7c6c85134939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+-------+-----+---+-----+\n",
      "|Year|Semester|Course|Credits|Grade|_c0|  _c1|\n",
      "+----+--------+------+-------+-----+---+-----+\n",
      "|2016|    Fall|IST346|      3|    A|  A|  4.0|\n",
      "|2016|    Fall|CHE111|      4|   A-| A-|3.666|\n",
      "|2016|    Fall|PSY120|      3|   B+| B+|3.333|\n",
      "|2016|    Fall|IST256|      3|    A|  A|  4.0|\n",
      "|2016|    Fall|ENG121|      3|   B+| B+|3.333|\n",
      "|2015|    Fall|IST101|      1|    A|  A|  4.0|\n",
      "|2015|    Fall|IST195|      3|    A|  A|  4.0|\n",
      "|2015|    Fall|IST233|      3|   B+| B+|3.333|\n",
      "|2015|    Fall|SOC101|      3|   A-| A-|3.666|\n",
      "|2015|    Fall|MAT221|      3|    C|  C|  2.0|\n",
      "|2016|  Spring|GEO110|      3|   B+| B+|3.333|\n",
      "|2016|  Spring|MAT222|      3|    A|  A|  4.0|\n",
      "|2016|  Spring|SOC121|      3|   C+| C+|2.333|\n",
      "|2016|  Spring|BIO240|      3|   B-| B-|2.666|\n",
      "|2017|  Spring|IST462|      3|    A|  A|  4.0|\n",
      "|2017|  Spring|MAT411|      3|    C|  C|  2.0|\n",
      "|2017|  Spring|SOC422|      3|   B-| B-|2.666|\n",
      "|2017|  Spring|ENV201|      3|   A-| A-|3.666|\n",
      "+----+--------+------+-------+-----+---+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+---+-----+\n",
      "|Year|Semester|Course|Credits|Grade|_c0|  _c1|\n",
      "+----+--------+------+-------+-----+---+-----+\n",
      "|2016|    Fall|IST346|      3|    A|  A|  4.0|\n",
      "|2016|    Fall|IST256|      3|    A|  A|  4.0|\n",
      "|2015|    Fall|IST101|      1|    A|  A|  4.0|\n",
      "|2015|    Fall|IST195|      3|    A|  A|  4.0|\n",
      "|2016|  Spring|MAT222|      3|    A|  A|  4.0|\n",
      "|2017|  Spring|IST462|      3|    A|  A|  4.0|\n",
      "|2016|    Fall|CHE111|      4|   A-| A-|3.666|\n",
      "|2015|    Fall|SOC101|      3|   A-| A-|3.666|\n",
      "|2017|  Spring|ENV201|      3|   A-| A-|3.666|\n",
      "|NULL|    NULL|  NULL|   NULL| NULL|  B|  3.0|\n",
      "|2016|    Fall|PSY120|      3|   B+| B+|3.333|\n",
      "|2016|    Fall|ENG121|      3|   B+| B+|3.333|\n",
      "|2015|    Fall|IST233|      3|   B+| B+|3.333|\n",
      "|2016|  Spring|GEO110|      3|   B+| B+|3.333|\n",
      "|2016|  Spring|BIO240|      3|   B-| B-|2.666|\n",
      "|2017|  Spring|SOC422|      3|   B-| B-|2.666|\n",
      "|2015|    Fall|MAT221|      3|    C|  C|  2.0|\n",
      "|2017|  Spring|MAT411|      3|    C|  C|  2.0|\n",
      "|2016|  Spring|SOC121|      3|   C+| C+|2.333|\n",
      "|NULL|    NULL|  NULL|   NULL| NULL| C-|1.666|\n",
      "+----+--------+------+-------+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades.join(gradepoints, grades.Grade == gradepoints._c0, \"inner\").show()\n",
    "\n",
    "# Nobody got a B or C-\n",
    "grades.join(gradepoints, grades.Grade == gradepoints._c0, \"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776c3f0-6ade-470c-a3b7-62dda46cce97",
   "metadata": {},
   "source": [
    "### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aae02694-1807-4c25-8621-b7aa3f0a548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "+----+--------+------+-------+-----+\n",
      "|Year|Semester|Course|Credits|Grade|\n",
      "+----+--------+------+-------+-----+\n",
      "|2016|    Fall|IST346|      3|    A|\n",
      "|2016|    Fall|CHE111|      4|   A-|\n",
      "|2016|    Fall|PSY120|      3|   B+|\n",
      "|2016|    Fall|IST256|      3|    A|\n",
      "|2016|    Fall|ENG121|      3|   B+|\n",
      "|2015|    Fall|IST101|      1|    A|\n",
      "|2015|    Fall|IST195|      3|    A|\n",
      "|2015|    Fall|IST233|      3|   B+|\n",
      "|2015|    Fall|SOC101|      3|   A-|\n",
      "|2015|    Fall|MAT221|      3|    C|\n",
      "|2016|  Spring|GEO110|      3|   B+|\n",
      "|2016|  Spring|MAT222|      3|    A|\n",
      "|2016|  Spring|SOC121|      3|   C+|\n",
      "|2016|  Spring|BIO240|      3|   B-|\n",
      "|2017|  Spring|IST462|      3|    A|\n",
      "|2017|  Spring|MAT411|      3|    C|\n",
      "|2017|  Spring|SOC422|      3|   B-|\n",
      "|2017|  Spring|ENV201|      3|   A-|\n",
      "+----+--------+------+-------+-----+\n",
      "\n",
      "Double the courses!\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   36|\n",
      "+-----+\n",
      "\n",
      "Filter out the duplicates\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   18|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "fallgrades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/fall*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "springgrades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/spring*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "fallgrades.show()\n",
    "springgrades.show()\n",
    "\n",
    "fallgrades.union(springgrades).show()\n",
    "\n",
    "print(\"Double the courses!\")\n",
    "grades.union(grades).groupBy().count().show()\n",
    "\n",
    "print(\"Filter out the duplicates\")\n",
    "grades.union(grades).distinct().groupby().count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbe3de-138d-4879-8b4b-b621b2cb72f5",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDF's)\n",
    "\n",
    "- User-defined functions allow us to write custom transformations. The process:\n",
    "\n",
    "1. Create python function, decorated for spark with `@func.udf(returnType=?)`, \n",
    "2. Apply function in `select()` or `withColumn()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d2cd81a-6c33-47bd-a4cc-46822be0dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+-------+-----+-----------+\n",
      "|Year|Semester|Course|Credits|Grade|       Term|\n",
      "+----+--------+------+-------+-----+-----------+\n",
      "|2016|    Fall|IST346|      3|    A|  2016-Fall|\n",
      "|2016|    Fall|CHE111|      4|   A-|  2016-Fall|\n",
      "|2016|    Fall|PSY120|      3|   B+|  2016-Fall|\n",
      "|2016|    Fall|IST256|      3|    A|  2016-Fall|\n",
      "|2016|    Fall|ENG121|      3|   B+|  2016-Fall|\n",
      "|2015|    Fall|IST101|      1|    A|  2015-Fall|\n",
      "|2015|    Fall|IST195|      3|    A|  2015-Fall|\n",
      "|2015|    Fall|IST233|      3|   B+|  2015-Fall|\n",
      "|2015|    Fall|SOC101|      3|   A-|  2015-Fall|\n",
      "|2015|    Fall|MAT221|      3|    C|  2015-Fall|\n",
      "|2016|  Spring|GEO110|      3|   B+|2016-Spring|\n",
      "|2016|  Spring|MAT222|      3|    A|2016-Spring|\n",
      "|2016|  Spring|SOC121|      3|   C+|2016-Spring|\n",
      "|2016|  Spring|BIO240|      3|   B-|2016-Spring|\n",
      "|2017|  Spring|IST462|      3|    A|2017-Spring|\n",
      "|2017|  Spring|MAT411|      3|    C|2017-Spring|\n",
      "|2017|  Spring|SOC422|      3|   B-|2017-Spring|\n",
      "|2017|  Spring|ENV201|      3|   A-|2017-Spring|\n",
      "+----+--------+------+-------+-----+-----------+\n",
      "\n",
      "+------+-------+\n",
      "|Course|InMajor|\n",
      "+------+-------+\n",
      "|IST346|   true|\n",
      "|CHE111|  false|\n",
      "|PSY120|  false|\n",
      "|IST256|   true|\n",
      "|ENG121|  false|\n",
      "|IST101|   true|\n",
      "|IST195|   true|\n",
      "|IST233|   true|\n",
      "|SOC101|  false|\n",
      "|MAT221|  false|\n",
      "|GEO110|  false|\n",
      "|MAT222|  false|\n",
      "|SOC121|  false|\n",
      "|BIO240|  false|\n",
      "|IST462|   true|\n",
      "|MAT411|  false|\n",
      "|SOC422|  false|\n",
      "|ENV201|  false|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def term(year, semester):\n",
    "    return f\"{year}-{semester}\"\n",
    "\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def inMajor(course):\n",
    "    return course.startswith(\"IST\")\n",
    "\n",
    "\n",
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "\n",
    "grades.withColumn(\"Term\", term( grades.Year, grades.Semester) ).show()\n",
    "\n",
    "grades.select(\"Course\", inMajor(grades.Course).alias(\"InMajor\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07975d9-082e-4783-8593-209b757d9708",
   "metadata": {},
   "source": [
    "## Nested Column Transformations\n",
    "\n",
    "- Sometimes the schema is nested with additional `StructType` or `ArrayType` fields.\n",
    "- For nested `StructType` you can use the object property accessor to get to the nested columns.\n",
    "- For nested `ArrayType` you can use the `explode()` function to flatten the nested data. when you explode an array, the parent values will repeat for each value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f36671c8-b035-4a7d-bc0c-24aa2434f617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_status: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- location: struct (nullable = true)\n",
      " |    |    |-- lat: double (nullable = true)\n",
      " |    |    |-- lng: double (nullable = true)\n",
      " |    |-- viewport: struct (nullable = true)\n",
      " |    |    |-- northeast: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- lng: double (nullable = true)\n",
      " |    |    |-- southwest: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- lng: double (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- icon_background_color: string (nullable = true)\n",
      " |-- icon_mask_base_uri: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- opening_hours: struct (nullable = true)\n",
      " |    |-- open_now: boolean (nullable = true)\n",
      " |-- photos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html_attributions: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- photo_reference: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- plus_code: struct (nullable = true)\n",
      " |    |-- compound_code: string (nullable = true)\n",
      " |    |-- global_code: string (nullable = true)\n",
      " |-- price_level: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- reference: string (nullable = true)\n",
      " |-- scope: string (nullable = true)\n",
      " |-- types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_ratings_total: long (nullable = true)\n",
      " |-- vicinity: string (nullable = true)\n",
      "\n",
      "+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+-----------+------+--------------------+------+--------------------+------------------+--------------------+\n",
      "|business_status|            geometry|                icon|icon_background_color|  icon_mask_base_uri|                name|opening_hours|              photos|            place_id|           plus_code|price_level|rating|           reference| scope|               types|user_ratings_total|            vicinity|\n",
      "+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+-----------+------+--------------------+------+--------------------+------------------+--------------------+\n",
      "|           NULL|{{43.0481221, -76...|https://maps.gsta...|              #7B9EB0|https://maps.gsta...|            Syracuse|         NULL|[{1080, [<a href=...|ChIJDZqXv5vz2YkRR...|                NULL|       NULL|  NULL|ChIJDZqXv5vz2YkRR...|GOOGLE|[locality, politi...|              NULL|            Syracuse|\n",
      "|    OPERATIONAL|{{43.0476078, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|Crowne Plaza Syra...|       {true}|[{2048, [<a href=...|ChIJXxPu66Tz2YkRr...|{2VX5+27 Syracuse...|       NULL|   4.1|ChIJXxPu66Tz2YkRr...|GOOGLE|[lodging, point_o...|              1153|701 East Genesee ...|\n",
      "|    OPERATIONAL|{{43.0476157, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|  The Parkview Hotel|       {true}|[{2584, [<a href=...|ChIJrWsN9KTz2YkRF...|{2VX5+2J Syracuse...|       NULL|   4.3|ChIJrWsN9KTz2YkRF...|GOOGLE|[lodging, point_o...|               350|713 East Genesee ...|\n",
      "|    OPERATIONAL|{{43.0472894, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|Jefferson Clinton...|       {true}|[{4096, [<a href=...|ChIJa_hOyrjz2YkRP...|{2RWW+WF Syracuse...|       NULL|   4.4|ChIJa_hOyrjz2YkRP...|GOOGLE|[lodging, point_o...|               397|416 South Clinton...|\n",
      "|    OPERATIONAL|{{43.0488846, -76...|https://maps.gsta...|              #909CE1|https://maps.gsta...|Courtyard by Marr...|       {true}|[{1192, [<a href=...|ChIJGzEmOsfz2YkRm...|{2RXV+HH Syracuse...|       NULL|   4.1|ChIJGzEmOsfz2YkRm...|GOOGLE|[lodging, point_o...|               396|300 West Fayette ...|\n",
      "+---------------+--------------------+--------------------+---------------------+--------------------+--------------------+-------------+--------------------+--------------------+--------------------+-----------+------+--------------------+------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Two places\n",
      "+--------------------+----------+---------------------+--------------------+\n",
      "|                name|       lat|geometry.location.lng|               types|\n",
      "+--------------------+----------+---------------------+--------------------+\n",
      "|            Syracuse|43.0481221|   -76.14742439999999|[locality, politi...|\n",
      "|Crowne Plaza Syra...|43.0476078|          -76.1417642|[lodging, point_o...|\n",
      "+--------------------+----------+---------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Same two places, one row per type\n",
      "+--------------------+----------+---------------------+-----------------+\n",
      "|                name|       lat|geometry.location.lng|             type|\n",
      "+--------------------+----------+---------------------+-----------------+\n",
      "|            Syracuse|43.0481221|   -76.14742439999999|         locality|\n",
      "|            Syracuse|43.0481221|   -76.14742439999999|        political|\n",
      "|Crowne Plaza Syra...|43.0476078|          -76.1417642|          lodging|\n",
      "|Crowne Plaza Syra...|43.0476078|          -76.1417642|point_of_interest|\n",
      "|Crowne Plaza Syra...|43.0476078|          -76.1417642|    establishment|\n",
      "+--------------------+----------+---------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Let's the the photo attributions\n",
      "+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|name                                                     |attributions                                                                                                                       |\n",
      "+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Syracuse                                                 |<a href=\"https://maps.google.com/maps/contrib/113924016854173339844\">Saddam sharhan</a>                                            |\n",
      "|Crowne Plaza Syracuse                                    |<a href=\"https://maps.google.com/maps/contrib/110257992175489236978\">Crowne Plaza Syracuse</a>                                     |\n",
      "|The Parkview Hotel                                       |<a href=\"https://maps.google.com/maps/contrib/107877625718574939160\">The Parkview Hotel</a>                                        |\n",
      "|Jefferson Clinton Suites                                 |<a href=\"https://maps.google.com/maps/contrib/107675618280941295230\">John H. Heinen</a>                                            |\n",
      "|Courtyard by Marriott Syracuse Downtown at Armory Square |<a href=\"https://maps.google.com/maps/contrib/104440568886738009728\">Courtyard by Marriott Syracuse Downtown at Armory Square</a>  |\n",
      "|Quality Inn & Suites Downtown                            |<a href=\"https://maps.google.com/maps/contrib/108484117399692538038\">SERF</a>                                                      |\n",
      "|Syracuse University                                      |<a href=\"https://maps.google.com/maps/contrib/105631489023550548135\">Akram Husam</a>                                               |\n",
      "|Collegian Hotel & Suites, Trademark Collection by Wyndham|<a href=\"https://maps.google.com/maps/contrib/100850734738657404193\">Solas Studios Photography</a>                                 |\n",
      "|Dinosaur Bar-B-Que                                       |<a href=\"https://maps.google.com/maps/contrib/113647926828363669179\">Dinosaur Bar-B-Que</a>                                        |\n",
      "|Hotel Skyler Syracuse, Tapestry Collection by Hilton     |<a href=\"https://maps.google.com/maps/contrib/103420373277684198141\">Hotel Skyler Syracuse, Tapestry Collection by Hilton</a>      |\n",
      "|Sheraton Syracuse University Hotel & Conference Center   |<a href=\"https://maps.google.com/maps/contrib/115126549326129000214\">Sheraton Syracuse University Hotel &amp; Conference Center</a>|\n",
      "|Syracuse Crunch Hockey Club                              |<a href=\"https://maps.google.com/maps/contrib/105026466240418454620\">Syracuse Crunch Hockey Club</a>                               |\n",
      "|Mulroy Civic Center                                      |<a href=\"https://maps.google.com/maps/contrib/102735218077833488460\">World B Free</a>                                              |\n",
      "|Crouse Hospital                                          |<a href=\"https://maps.google.com/maps/contrib/102059532679223038401\">Ed Helinski</a>                                               |\n",
      "|Pastabilities                                            |<a href=\"https://maps.google.com/maps/contrib/103039051803802326412\">Pastabilities</a>                                             |\n",
      "|Syracuse VA Medical Center                               |<a href=\"https://maps.google.com/maps/contrib/109810061360867197750\">Tiffany Allington</a>                                         |\n",
      "+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "places = spark.read.json(\"file:///home/jovyan/datasets/json-samples/google-places.json\", multiLine=True)\n",
    "places.printSchema()\n",
    "places.show(5)\n",
    "\n",
    "print(\"Two places\")\n",
    "places.select('name','geometry.location.lat',places.geometry.location.lng, places['types']).show(2)\n",
    "\n",
    "print(\"Same two places, one row per type\")\n",
    "places.select('name','geometry.location.lat',places.geometry.location.lng, explode(places.types).alias(\"type\") ).show(5)\n",
    "\n",
    "print(\"Let's the the photo attributions\")\n",
    "places.select('name', explode( places.photos ).alias(\"col\") ) \\\n",
    "    .select(\"name\", explode(\"col.html_attributions\").alias(\"attributions\") ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a7ec6-e881-4f70-b88b-bb31160e2e4e",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "The `explain()` function will demonstrate the execution plan of the spark transformations. \n",
    "This is useful for understanding how the DAG processes the transformations. \n",
    "It should be noted that they are not processed in the order as written but instead processed  as optimized by spark.\n",
    "\n",
    "Notice in this example the last transformation is to filter the Year to 2016. In the Physical plan, this is one of the first transoformations. (You read the transformation graph from bottom to top).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f85d052-16f6-454b-ac34-c0ba95614a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Year#2906 ASC NULLS FIRST, Semester#2907 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Year#2906 ASC NULLS FIRST, Semester#2907 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2280]\n",
      "      +- HashAggregate(keys=[Year#2906, Semester#2907], functions=[count(1), sum(Credits#2909)])\n",
      "         +- Exchange hashpartitioning(Year#2906, Semester#2907, 200), ENSURE_REQUIREMENTS, [plan_id=2277]\n",
      "            +- HashAggregate(keys=[Year#2906, Semester#2907], functions=[partial_count(1), partial_sum(Credits#2909)])\n",
      "               +- Project [_c0#2896 AS Year#2906, _c1#2897 AS Semester#2907, _c3#2899 AS Credits#2909]\n",
      "                  +- Filter (isnotnull(_c0#2896) AND (_c0#2896 = 2016))\n",
      "                     +- FileScan csv [_c0#2896,_c1#2897,_c3#2899] Batched: false, DataFilters: [isnotnull(_c0#2896), (_c0#2896 = 2016)], Format: CSV, Location: InMemoryFileIndex(4 paths)[file:/home/jovyan/datasets/grades/fall2015.tsv, file:/home/jovyan/data..., PartitionFilters: [], PushedFilters: [IsNotNull(_c0), EqualTo(_c0,2016)], ReadSchema: struct<_c0:int,_c1:string,_c3:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grades = spark.read.option(\"header\",False).option(\"inferSchema\", True).option(\"sep\", \"\\t\").csv(\"file:///home/jovyan/datasets/grades/*.tsv\").toDF(\"Year\", \"Semester\", \"Course\", \"Credits\", \"Grade\")\n",
    "termcredits = grades.groupBy(\"Year\", \"Semester\").agg( \\\n",
    "    count(\"*\").alias(\"CourseCount\"), sum(\"Credits\").alias(\"TotalCredits\") \\\n",
    "    ).sort(\"Year\",col(\"Semester\").desc())\n",
    "final = termcredits.filter(\"Year=2016\")\n",
    "final.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7d34881-5ef4-4935-af45-a26a7eb56556",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = grades.filter(\"year = 2016\")\\\n",
    "    .filter(grades.Semester == \"Fall\")\\\n",
    "    .sort(\"Course\") \\\n",
    "    .select(\"Course\", grades.Credits, grades[\"Grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94c110de-f775-4c90-9858-a53ef166b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Course#2908 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Course#2908 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2294]\n",
      "      +- Project [_c2#2898 AS Course#2908, _c3#2899 AS Credits#2909, _c4#2900 AS Grade#2910]\n",
      "         +- Filter (((isnotnull(_c0#2896) AND isnotnull(_c1#2897)) AND (_c0#2896 = 2016)) AND (_c1#2897 = Fall))\n",
      "            +- FileScan csv [_c0#2896,_c1#2897,_c2#2898,_c3#2899,_c4#2900] Batched: false, DataFilters: [isnotnull(_c0#2896), isnotnull(_c1#2897), (_c0#2896 = 2016), (_c1#2897 = Fall)], Format: CSV, Location: InMemoryFileIndex(4 paths)[file:/home/jovyan/datasets/grades/fall2015.tsv, file:/home/jovyan/data..., PartitionFilters: [], PushedFilters: [IsNotNull(_c0), IsNotNull(_c1), EqualTo(_c0,2016), EqualTo(_c1,Fall)], ReadSchema: struct<_c0:int,_c1:string,_c2:string,_c3:int,_c4:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ce023ac-da49-479f-beb7-faab9b28e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = grades.sort(\"Course\") \\\n",
    "    .filter(grades.Semester == \"Fall\")\\\n",
    "    .select(\"Course\", grades.Credits, grades[\"Grade\"])\\\n",
    "    .filter(\"year = 2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3fc590f-934c-46f1-87d4-429855a377f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Course#2908 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Course#2908 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2308]\n",
      "      +- Project [_c2#2898 AS Course#2908, _c3#2899 AS Credits#2909, _c4#2900 AS Grade#2910]\n",
      "         +- Filter (((isnotnull(_c1#2897) AND isnotnull(_c0#2896)) AND (_c1#2897 = Fall)) AND (_c0#2896 = 2016))\n",
      "            +- FileScan csv [_c0#2896,_c1#2897,_c2#2898,_c3#2899,_c4#2900] Batched: false, DataFilters: [isnotnull(_c1#2897), isnotnull(_c0#2896), (_c1#2897 = Fall), (_c0#2896 = 2016)], Format: CSV, Location: InMemoryFileIndex(4 paths)[file:/home/jovyan/datasets/grades/fall2015.tsv, file:/home/jovyan/data..., PartitionFilters: [], PushedFilters: [IsNotNull(_c1), IsNotNull(_c0), EqualTo(_c1,Fall), EqualTo(_c0,2016)], ReadSchema: struct<_c0:int,_c1:string,_c2:string,_c3:int,_c4:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d38a668e-c62d-4c0d-9aa2-e26314f92908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: string, symbol: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e36fe964-927f-4292-b82b-60e591d35a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  price|symbol|\n",
      "+-------+------+\n",
      "|  45.11|  TWTR|\n",
      "|   78.0|   NET|\n",
      "| 126.82|  AAPL|\n",
      "| 128.39|   IBM|\n",
      "| 212.55|  MSFT|\n",
      "| 251.11|    FB|\n",
      "|  497.0|  NFLX|\n",
      "|  823.8|  TSLA|\n",
      "|1725.05|  GOOG|\n",
      "|3098.12|  AMZN|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "#df.withColumn(\"price\", df.price.cast(DoubleType())).printSchema().sort(df[\"price\"]).toPandas()\n",
    "\n",
    "df.sort(df.price.cast(\"Float\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbd5eb-9f7f-4843-9900-380e7ff30b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a2d43-7dc6-4f60-86ec-c2cc2d0a7f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
