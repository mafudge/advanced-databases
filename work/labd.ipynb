{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8742eb-df86-46a8-b1a8-c008346d479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# MINIO CONFIGURATION\n",
    "s3_host = \"minio\"\n",
    "s3_url = f\"http://{s3_host}:9000\"\n",
    "s3_key = \"minio\"\n",
    "s3_secret = \"SU2orange!\"\n",
    "s3_bucket = \"labd\"\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.4\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_url) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", s3_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a09df75-1359-40bc-b14a-3686b401d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping Headers...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o44.text.\n: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:592)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:556)\n\tat org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:52)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:256)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClasses(Configuration.java:2663)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:589)\n\t... 31 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStripping Headers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#logs2 = logs1.filter(~logs1['value'].startswith(\"#\") ) \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs_in\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue not like \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext(logs_out)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:615\u001b[0m, in \u001b[0;36mDataFrameReader.text\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    613\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [paths]\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o44.text.\n: java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:592)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:556)\n\tat org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:52)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:256)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClasses(Configuration.java:2663)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:589)\n\t... 31 more\n"
     ]
    }
   ],
   "source": [
    "logs_in = f\"s3a://{s3_bucket}/logs/*.log\"\n",
    "logs_out = f\"s3a://{s3_bucket}/logs_no_header\"\n",
    "iplookup_in = f\"s3a://{s3_bucket}/iplookup/iplookup.json\"\n",
    "cleanedlogs_out = f\"s3a://{s3_bucket}/cleaned-logs.parquet\"\n",
    "\n",
    "print(\"Stripping Headers...\")\n",
    "#logs2 = logs1.filter(~logs1['value'].startswith(\"#\") ) \n",
    "spark.read.text(logs_in)\\\n",
    "    .filter(\"value not like '#%'\")\\\n",
    "    .write.mode(\"Overwrite\").text(logs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585147a-8e89-4625-9c21-8004826dc03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(logs_in)\n",
    "#df.select(df.value.substr(0,1),df.value.substr(0,1) != \"#\" ).show()\n",
    "df.filter(df.value.substr(0,1) != \"#\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09ab6a-914e-4bf0-85e6-5ecc3ca4189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading stripped logs...\")\n",
    "logs = spark.read.option(\"header\",False).option(\"inferSchema\",True).option(\"sep\",\" \").csv(logs_out)\\\n",
    "    .toDF(\"date\",\"time\", \"serverip\", \"method\", \"uri\", \"querystirng\", \"port\", \"username\", \"clientip\", \"useragent\", \"referrer\", \"statuscode\", \"a\",\"b\",\"c\")\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b4207df-faa4-4e21-bb26-1d1e34187d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IP lookups...\n",
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IP lookups...\")\n",
    "ip = spark.read.option(\"multiline\", True).json(iplookup_in)\n",
    "ip = ip.select(\"ip\",ip.geography.city.alias(\"city\"), \"geography.state\", \"geography.country\", \"location.lat\", \"location.lng\")\n",
    "\n",
    "ip.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9d9b252-ab71-431f-a973-456079fac87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geography: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- lat: double (nullable = true)\n",
      " |    |-- lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9fac134-16c0-43a8-af94-d7294a2bbe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------+\n",
      "|location.lat|location.lng|geography.state|\n",
      "+------------+------------+---------------+\n",
      "|   40.657602|  -73.583184|             NY|\n",
      "|   43.048122|  -76.147424|             NY|\n",
      "|   40.712784|  -74.005941|             NY|\n",
      "|   43.048122|  -76.147424|             NY|\n",
      "+------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = ip.select(ip.location.lat, ip.location.lng, ip.geography.state)\n",
    "b = a.where(\"geography.state = 'NY'\")\n",
    "b.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95d6730c-a478-4199-a577-6d16b441e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO,No,no... dont do this!\n",
    "spark.read.text(logs_out).toPandas().to_csv(\"file.csv\")\n",
    "\n",
    "#python generator\n",
    "#loop that does a yeild\n",
    "\n",
    "# with open(\"file\",\"r\") as f:\n",
    "#     for line in f.readlines():\n",
    "#         print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65c0df54-9936-4f9a-b759-381e85f4cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stripped logs...\n",
      "Loading IP lookups...\n",
      "Join DataFrames...\n",
      "Save as Parquet...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Join DataFrames...\") \n",
    "comb = ip.join(logs, on = ip.ip == logs.clientip, how=\"inner\")\n",
    "\n",
    "print(\"Save as Parquet...\")\n",
    "comb.write.mode(\"Overwrite\").parquet(cleanedlogs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e519a557-d494-4901-a6c2-93f13c324487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|#Software: Micros...|\n",
      "|       #Version: 1.0|\n",
      "|#Date: 2016-02-11...|\n",
      "|#Fields: date tim...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#3\n",
    "logs_in = f\"s3a://{s3_bucket}/logs/*.log\"\n",
    "logs1 = spark.read.text(logs_in)\n",
    "logs1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36334f4a-d764-4dd6-902a-5fc3fc1bc038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "|2016-02-11 17:16:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 \n",
    "#logs2 =  logs1.filter(\"value not like '#%'\")\n",
    "logs2 = logs1.filter(~logs1.value.startswith(\"#\"))\n",
    "logs2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43201dc-3c77-4272-ae05-9c50f99d5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 \n",
    "logs_out = f\"s3a://{s3_bucket}/logs_no_header\"\n",
    "logs2.write.text(logs_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72105006-ce5f-4a7c-aa73-00fcde8f3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------+------+--------------------+-----------+----+--------+-----------+--------------------+--------------------+----------+\n",
      "|      date|    time|      serverip|method|                 uri|querystirng|port|username|   clientip|           useragent|            referrer|statuscode|\n",
      "+----------+--------+--------------+------+--------------------+-----------+----+--------+-----------+--------------------+--------------------+----------+\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|                   /|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|                   -|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Content/jquery-u...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Plugins/Widgets....|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Plugins/Widgets....|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/jquery.v...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/jquery.v...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Themes/DefaultCl...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/jquery-m...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/public.c...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/public.a...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Plugins/Widgets....|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/jquery-u...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Scripts/jquery-1...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/content/images/t...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/Themes/DefaultCl...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/content/images/t...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/content/images/t...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/content/images/t...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/content/images/t...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "|2016-02-11|17:16:13|128.230.247.37|   GET|/content/images/t...|          -|  80|       -|215.82.23.2|Mozilla/5.0+(Wind...|http://group0.ist...|       200|\n",
      "+----------+--------+--------------+------+--------------------+-----------+----+--------+-----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "logs3 = spark.read.option(\"header\",False).option(\"inferSchema\",True).option(\"sep\",\" \").csv(logs_out)\\\n",
    "    .toDF(\"date\",\"time\", \"serverip\", \"method\", \"uri\", \"querystirng\", \"port\", \"username\", \"clientip\", \"useragent\", \"referrer\", \"statuscode\", \"a\",\"b\",\"c\")\\\n",
    "    .select(\"date\",\"time\", \"serverip\", \"method\", \"uri\", \"querystirng\", \"port\", \"username\", \"clientip\", \"useragent\", \"referrer\", \"statuscode\")\n",
    "logs3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "208c8bcf-fd9a-4992-aa9b-104d4525e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+\n",
      "|           geography|             ip|            location|\n",
      "+--------------------+---------------+--------------------+\n",
      "|   {Dulles, USA, VA}|  172.189.252.8|{38.955855, -77.4...|\n",
      "| {Columbus, USA, OH}|    215.82.23.2|{39.961176, -82.9...|\n",
      "|{Cleveland, USA, OH}|    98.29.25.44|{41.49932, -81.69...|\n",
      "| {Freeport, USA, NY}|  68.199.40.156|{40.657602, -73.5...|\n",
      "|{Salt Lake City, ...|155.100.169.152|{40.760779, -111....|\n",
      "|   {Dallas, USA, TX}|   38.68.15.223|{32.776664, -96.7...|\n",
      "|    {Tampa, USA, FL}|   70.209.14.54|{27.950575, -82.4...|\n",
      "|{Arlington, USA, VA}|   74.111.6.173|{38.87997, -77.10...|\n",
      "| {Syracuse, USA, NY}|128.230.122.180|{43.048122, -76.1...|\n",
      "| {New York, USA, NY}|128.122.140.238|{40.712784, -74.0...|\n",
      "|  {Raleigh, USA, NC}| 56.216.127.219|{35.77959, -78.63...|\n",
      "|{Jersey City, USA...| 54.114.107.209|{40.728157, -74.0...|\n",
      "| {Syracuse, USA, NY}|   74.111.18.59|{43.048122, -76.1...|\n",
      "|{Los Angeles, USA...|    8.37.70.170|{34.052234, -118....|\n",
      "|{Los Angeles, USA...|     8.37.70.77|{34.052234, -118....|\n",
      "|{Los Angeles, USA...|    8.37.70.112|{34.052234, -118....|\n",
      "|{Los Angeles, USA...|    8.37.70.226|{34.052234, -118....|\n",
      "|{Los Angeles, USA...|     8.37.70.99|{34.052234, -118....|\n",
      "|{Los Angeles, USA...|     8.37.71.43|{34.052234, -118....|\n",
      "|{Los Angeles, USA...|     8.37.71.25|{34.052234, -118....|\n",
      "+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "iplookup_in = f\"s3a://{s3_bucket}/iplookup/iplookup.json\"\n",
    "ips1 = spark.read.option(\"multiline\", True).json(iplookup_in)\n",
    "ips1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13eaf95f-a716-4e85-87bc-f2f1470c4f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----+-------+---------+-----------+\n",
      "|             ip|          city|state|country|      lat|        lng|\n",
      "+---------------+--------------+-----+-------+---------+-----------+\n",
      "|  172.189.252.8|        Dulles|   VA|    USA|38.955855| -77.447819|\n",
      "|    215.82.23.2|      Columbus|   OH|    USA|39.961176| -82.998794|\n",
      "|    98.29.25.44|     Cleveland|   OH|    USA| 41.49932| -81.694361|\n",
      "|  68.199.40.156|      Freeport|   NY|    USA|40.657602| -73.583184|\n",
      "|155.100.169.152|Salt Lake City|   UT|    USA|40.760779|-111.891047|\n",
      "|   38.68.15.223|        Dallas|   TX|    USA|32.776664| -96.796988|\n",
      "|   70.209.14.54|         Tampa|   FL|    USA|27.950575| -82.457178|\n",
      "|   74.111.6.173|     Arlington|   VA|    USA| 38.87997|  -77.10677|\n",
      "|128.230.122.180|      Syracuse|   NY|    USA|43.048122| -76.147424|\n",
      "|128.122.140.238|      New York|   NY|    USA|40.712784| -74.005941|\n",
      "| 56.216.127.219|       Raleigh|   NC|    USA| 35.77959| -78.638179|\n",
      "| 54.114.107.209|   Jersey City|   NJ|    USA|40.728157| -74.077642|\n",
      "|   74.111.18.59|      Syracuse|   NY|    USA|43.048122| -76.147424|\n",
      "|    8.37.70.170|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "|     8.37.70.77|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "|    8.37.70.112|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "|    8.37.70.226|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "|     8.37.70.99|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "|     8.37.71.43|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "|     8.37.71.25|   Los Angeles|   CA|    USA|34.052234|-118.243685|\n",
      "+---------------+--------------+-----+-------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8 \n",
    "ips2 = ips1.select(\"ip\",\"geography.city\", \"geography.state\", \"geography.country\", \"location.lat\", ips1.location.lng.alias(\"lng\") )\n",
    "ips2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d557559-1d99-4cba-a328-c2a99f842b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Content/jquery-ui-themes/smoothness/jquery-ui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Plugins/Widgets.NivoSlider/Content/nivoslider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Plugins/Widgets.NivoSlider/Content/nivoslider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Scripts/jquery.validate.unobtrusive.min.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>/Themes/DefaultClean/Content/images/compare-bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>/Themes/DefaultClean/Content/images/rating1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>/Themes/DefaultClean/Content/images/rating2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>/Themes/DefaultClean/Content/images/social-spr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>/Content/jquery-ui-themes/smoothness/images/ui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1122 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    uri\n",
       "0                                                     /\n",
       "1     /Content/jquery-ui-themes/smoothness/jquery-ui...\n",
       "2     /Plugins/Widgets.NivoSlider/Content/nivoslider...\n",
       "3     /Plugins/Widgets.NivoSlider/Content/nivoslider...\n",
       "4           /Scripts/jquery.validate.unobtrusive.min.js\n",
       "...                                                 ...\n",
       "1117  /Themes/DefaultClean/Content/images/compare-bu...\n",
       "1118    /Themes/DefaultClean/Content/images/rating1.png\n",
       "1119    /Themes/DefaultClean/Content/images/rating2.png\n",
       "1120  /Themes/DefaultClean/Content/images/social-spr...\n",
       "1121  /Content/jquery-ui-themes/smoothness/images/ui...\n",
       "\n",
       "[1122 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9\n",
    "comb1 = ips2.join(logs3, on = ips2.ip == logs3.clientip, how=\"inner\")\n",
    "comb1.select(\"uri\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4feed0f-e89a-4042-9ca7-7f463f96ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedlogs_out = f\"s3a://{s3_bucket}/cleaned-logs.parquet\"\n",
    "comb1.write.mode(\"Overwrite\").parquet(cleanedlogs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c5bcd-1702-4e19-b5fc-d9d48b27bb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
