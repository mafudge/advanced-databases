{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b29fae-308b-469f-9088-e1990260ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (1.0.0)\n",
      "Collecting pyspark<3.2.0,>=3.1.0 (from delta-spark)\n",
      "  Using cached pyspark-3.1.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=3.10.0->delta-spark) (3.20.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.2.0,>=3.1.0->delta-spark) (0.10.9)\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.3\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed pyspark-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46700bfe-b483-462b-ba94-8a1601ed42cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.utils' has no attribute 'convert_exception'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      5\u001b[0m builder \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjupyter-pyspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-avro_2.12:3.1.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://minio:9000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.extensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.delta.sql.DeltaSparkSessionExtension\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.spark_catalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/delta/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright (2021) The Delta Lake Project Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpip_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_spark_with_delta_pip\n\u001b[1;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeltaTable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigure_spark_with_delta_pip\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/delta/tables.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright (2021) The Delta Lake Project Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401; pylint: disable=unused-variable\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m since\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, DataFrame, functions, SparkSession\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/delta/exceptions.py:152\u001b[0m\n\u001b[1;32m    148\u001b[0m     utils\u001b[38;5;241m.\u001b[39mconvert_exception \u001b[38;5;241m=\u001b[39m convert_delta_exception\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _delta_exception_patched:\n\u001b[0;32m--> 152\u001b[0m     \u001b[43m_patch_convert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     _delta_exception_patched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/delta/exceptions.py:140\u001b[0m, in \u001b[0;36m_patch_convert_exception\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_patch_convert_exception\u001b[39m():\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Patch PySpark's exception convert method to convert Delta's Scala concurrent exceptions to the\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    corresponding Python exceptions.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     convert_sql_exception \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_exception\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_delta_exception\u001b[39m(e):\n\u001b[1;32m    143\u001b[0m         delta_exception \u001b[38;5;241m=\u001b[39m _convert_delta_exception(e)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.utils' has no attribute 'convert_exception'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"jupyter-pyspark\") \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-avro_2.12:3.1.2\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691581a7-2064-4d58-a6f8-f0e58ee4e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9c315-e23a-41ac-86a5-3bd9a6473f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a7dd-ff43-4d98-81d7-9b303ad3b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cc11a-e379-499b-95f9-2fd34234b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d95e9-7b62-49c0-a601-593ef481a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c2f3cb0-8123-4644-a813-df25e4b4c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta-table`\")\n",
    "latest_version = history.selectExpr(\"max(version)\").collect()\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version[0][0]).load(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b71c50f1-3fa3-4a9d-90e8-c07737134a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  9|\n",
      "|  5|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f76741dd-e17c-48d2-884c-48dc700515a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      1|2022-12-30 18:35:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|          null|        false|{numFiles -> 6, n...|        null|\n",
      "|      0|2022-12-30 18:35:...|  null|    null|    WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|          null|         true|{numFiles -> 6, n...|        null|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a77f1-4a0c-4932-93e9-a8037d15be4f",
   "metadata": {},
   "source": [
    "## More Real-World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816d5417-431c-4f8b-8a42-4862a0ed8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial Table\n",
    "people = spark.read.json(f\"s3a://delta/people/*.json\") # Read From S3\n",
    "people.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://delta/people-table\") # Write to DeltaTable on D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07140d82-9eb8-4817-ae86-36d1a1beff30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>Jingle</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>Heimer</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>Jacob</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>Smith</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  weight\n",
       "0   22   3  Jingle   188.0\n",
       "1   27   4  Heimer   201.0\n",
       "2   35   2   Jacob   166.0\n",
       "3   30   1    John   175.0\n",
       "4   38   5   Smith     NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01274461-b2d0-4cde-b7b3-bbfc168cd9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`s3a://delta/people-table`\")\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5c9972f-93ca-4e6a-a289-894eb2abc6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable = DeltaTable.forPath(spark,f\"s3a://delta/people-table\") # Get as Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "180b3ab0-4518-487b-8a5d-c263b8e24dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03171712-a4c4-462c-8c6e-1e3b6c83174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable.update(condition=\"name='John'\", set= { 'weight' : \"177\" }) #Update will re-write for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5678e1d-c29e-4822-a0d4-0c1fdfe9fac2",
   "metadata": {},
   "source": [
    "## History of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a691226e-0d50-4196-b828-07cd354bd074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69d49d6b-9fd6-442c-8ac0-9d92fad99641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`s3a://delta/people-table`\")\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01560bb1-e566-4cdb-bb06-d4529acce92f",
   "metadata": {},
   "source": [
    "## Direct delta query in Spark SQL, similar to apache drill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7731ee-fe61-4119-8318-90fa83854d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"select * from delta.`s3a://delta/people-table`\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8291cbf-6912-4a51-bf87-9289290cc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.merge("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80eb0fd-dbd9-4b3d-b867-d213add04f5f",
   "metadata": {},
   "source": [
    "## Use with Spark SQL by registering as a temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b819783f-865f-42cf-abcc-8e85fbc9d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable.toDF().createOrReplaceTempView(\"peopleTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd20c0a2-825c-45f3-8e44-238e7098e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from peopleTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042874db-17e4-45d7-9a58-9e6e4820349a",
   "metadata": {},
   "source": [
    "## Upsert / Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2ac13ac-f664-46fe-ae6e-a46a64a59103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|weight|\n",
      "+---+-----+---+------+\n",
      "|  1|Johnz| 31|   179|\n",
      "|  6|  His| 56|   156|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Johnz\", 31,179) ,(6, \"His\", 56, 156)]\n",
    "cols = [\"id\",\"name\",\"age\",\"weight\"]\n",
    "changes = spark.createDataFrame(data = data, schema = cols)\n",
    "changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81f3bb04-a836-4d17-bf85-5fdfb8322a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable.alias(\"tgt\").merge(changes.alias(\"src\"), condition = peopleTable.toDF().id == changes.id) \\\n",
    "    .whenMatchedUpdate( set = { \"name\" : changes.name, \"age\" : \"src.age\", \"weight\" : changes['weight'] } ) \\\n",
    "    .whenNotMatchedInsert( values = { \"id\" : \"src.id\", \"name\": \"src.name\", \"age\": \"src.age\", \"weight\" : \"src.weight\" })\\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4975a684-ec11-459f-891a-093f82930dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   179|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e508bee4-9a72-49c2-8224-34604ccdc6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|     13|2022-12-30 19:04:26|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|         12|          null|        false|{numTargetRowsCop...|        null|\n",
      "|     12|2022-12-30 19:04:20|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|         11|          null|        false|{numTargetRowsCop...|        null|\n",
      "|     11|2022-12-30 19:03:56|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|         10|          null|        false|{numTargetRowsCop...|        null|\n",
      "|     10|2022-12-30 19:03:48|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|          9|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      9|2022-12-30 19:02:51|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          8|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      8|2022-12-30 19:02:42|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          7|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      7|2022-12-30 19:02:28|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          6|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      6|2022-12-30 19:01:41|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          5|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      5|2022-12-30 19:01:30|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          4|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221e6b1-f27e-474f-947e-82e83de21688",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "Important to know this is a delta feature and not a spark feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "69bb6a34-201c-40a4-946c-720be412114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-30 18:30:26\").load(\"s3a://delta/people-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "16bf5e93-e607-43a8-8954-6b66e3602254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"s3a://delta/people-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "14c2eb93-cfd1-479a-ab28-48f4649cab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   179|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"timestampAfter\", \"2022-12-30 18:30:26\").load(\"s3a://delta/people-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0247347-b2bc-42b2-a2a9-9f3d509c3391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION: 13\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   179|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 12\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 31|  1| Johnz|   178|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 11\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 10\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 31|  1| Johns|   178|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 9\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 31|  1| Johns|   178|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 8\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 7\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 6\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 5\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 4\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 3\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 2\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 1\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 0\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = peopleTable.history()\n",
    "\n",
    "for row in history.collect():\n",
    "    ver = row.version\n",
    "    df = spark.read.format(\"delta\").option(\"versionAsOf\", ver).load(\"s3a://delta/people-table\")\n",
    "    print(f\"VERSION: {ver}\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a29d4-9507-4175-aad5-8c0870d9c344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
