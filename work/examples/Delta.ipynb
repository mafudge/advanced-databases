{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b29fae-308b-469f-9088-e1990260ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==1.0\n",
      "  Downloading delta_spark-1.0.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyspark<3.2.0,>=3.1.0 in /usr/local/spark-3.1.2-bin-hadoop3.2/python (from delta-spark==1.0) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.10.0 in /opt/conda/lib/python3.9/site-packages (from delta-spark==1.0) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=3.10.0->delta-spark==1.0) (3.5.0)\n",
      "Collecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: py4j, delta-spark\n",
      "Successfully installed delta-spark-1.0.0 py4j-0.10.9\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark==1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46700bfe-b483-462b-ba94-8a1601ed42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4f0399a-8f35-413e-a42a-62c77e751c07;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (194ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (107ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (47ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (45ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (20ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (24ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (757ms)\n",
      ":: resolution report :: resolve 3525ms :: artifacts dl 1228ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   8   |   8   |   0   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4f0399a-8f35-413e-a42a-62c77e751c07\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (15452kB/17ms)\n",
      "22/12/30 18:19:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"jupyter-pyspark\") \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.1.2,org.apache.spark:spark-avro_2.12:3.1.2\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"SU2orange!\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "691581a7-2064-4d58-a6f8-f0e58ee4e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ab9c315-e23a-41ac-86a5-3bd9a6473f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  0|\n",
      "|  4|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae58a7dd-ff43-4d98-81d7-9b303ad3b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5cc11a-e379-499b-95f9-2fd34234b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  8|\n",
      "|  9|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9d95e9-7b62-49c0-a601-593ef481a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  8|\n",
      "|  9|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c2f3cb0-8123-4644-a813-df25e4b4c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta-table`\")\n",
    "latest_version = history.selectExpr(\"max(version)\").collect()\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version[0][0]).load(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b71c50f1-3fa3-4a9d-90e8-c07737134a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  9|\n",
      "|  5|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f76741dd-e17c-48d2-884c-48dc700515a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      1|2022-12-30 18:35:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|          null|        false|{numFiles -> 6, n...|        null|\n",
      "|      0|2022-12-30 18:35:...|  null|    null|    WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|          null|         true|{numFiles -> 6, n...|        null|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a77f1-4a0c-4932-93e9-a8037d15be4f",
   "metadata": {},
   "source": [
    "## More Real-World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816d5417-431c-4f8b-8a42-4862a0ed8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial Table\n",
    "people = spark.read.json(f\"s3a://delta/people/*.json\") # Read From S3\n",
    "people.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://delta/people-table\") # Write to DeltaTable on D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07140d82-9eb8-4817-ae86-36d1a1beff30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>Jingle</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>Heimer</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>Jacob</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>Smith</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  weight\n",
       "0   22   3  Jingle   188.0\n",
       "1   27   4  Heimer   201.0\n",
       "2   35   2   Jacob   166.0\n",
       "3   30   1    John   175.0\n",
       "4   38   5   Smith     NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01274461-b2d0-4cde-b7b3-bbfc168cd9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`s3a://delta/people-table`\")\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5c9972f-93ca-4e6a-a289-894eb2abc6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable = DeltaTable.forPath(spark,f\"s3a://delta/people-table\") # Get as Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "180b3ab0-4518-487b-8a5d-c263b8e24dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03171712-a4c4-462c-8c6e-1e3b6c83174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable.update(condition=\"name='John'\", set= { 'weight' : \"177\" }) #Update will re-write for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5678e1d-c29e-4822-a0d4-0c1fdfe9fac2",
   "metadata": {},
   "source": [
    "## History of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a691226e-0d50-4196-b828-07cd354bd074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69d49d6b-9fd6-442c-8ac0-9d92fad99641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`s3a://delta/people-table`\")\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01560bb1-e566-4cdb-bb06-d4529acce92f",
   "metadata": {},
   "source": [
    "## Direct delta query in Spark SQL, similar to apache drill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7731ee-fe61-4119-8318-90fa83854d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"select * from delta.`s3a://delta/people-table`\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8291cbf-6912-4a51-bf87-9289290cc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.merge("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80eb0fd-dbd9-4b3d-b867-d213add04f5f",
   "metadata": {},
   "source": [
    "## Use with Spark SQL by registering as a temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b819783f-865f-42cf-abcc-8e85fbc9d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable.toDF().createOrReplaceTempView(\"peopleTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd20c0a2-825c-45f3-8e44-238e7098e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from peopleTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042874db-17e4-45d7-9a58-9e6e4820349a",
   "metadata": {},
   "source": [
    "## Upsert / Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2ac13ac-f664-46fe-ae6e-a46a64a59103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|weight|\n",
      "+---+-----+---+------+\n",
      "|  1|Johnz| 31|   179|\n",
      "|  6|  His| 56|   156|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Johnz\", 31,179) ,(6, \"His\", 56, 156)]\n",
    "cols = [\"id\",\"name\",\"age\",\"weight\"]\n",
    "changes = spark.createDataFrame(data = data, schema = cols)\n",
    "changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81f3bb04-a836-4d17-bf85-5fdfb8322a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleTable.alias(\"tgt\").merge(changes.alias(\"src\"), condition = peopleTable.toDF().id == changes.id) \\\n",
    "    .whenMatchedUpdate( set = { \"name\" : changes.name, \"age\" : \"src.age\", \"weight\" : changes['weight'] } ) \\\n",
    "    .whenNotMatchedInsert( values = { \"id\" : \"src.id\", \"name\": \"src.name\", \"age\": \"src.age\", \"weight\" : \"src.weight\" })\\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4975a684-ec11-459f-891a-093f82930dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   179|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e508bee4-9a72-49c2-8224-34604ccdc6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "|     13|2022-12-30 19:04:26|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|         12|          null|        false|{numTargetRowsCop...|        null|\n",
      "|     12|2022-12-30 19:04:20|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|         11|          null|        false|{numTargetRowsCop...|        null|\n",
      "|     11|2022-12-30 19:03:56|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|         10|          null|        false|{numTargetRowsCop...|        null|\n",
      "|     10|2022-12-30 19:03:48|  null|    null|    MERGE|{predicate -> (`i...|null|    null|     null|          9|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      9|2022-12-30 19:02:51|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          8|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      8|2022-12-30 19:02:42|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          7|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      7|2022-12-30 19:02:28|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          6|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      6|2022-12-30 19:01:41|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          5|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      5|2022-12-30 19:01:30|  null|    null|    MERGE|{predicate -> (tg...|null|    null|     null|          4|          null|        false|{numTargetRowsCop...|        null|\n",
      "|      4|2022-12-30 18:37:14|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          3|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      3|2022-12-30 18:36:33|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      2|2022-12-30 18:36:05|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 5, n...|        null|\n",
      "|      1|2022-12-30 18:33:24|  null|    null|   UPDATE|{predicate -> (na...|null|    null|     null|          0|          null|        false|{numRemovedFiles ...|        null|\n",
      "|      0|2022-12-30 18:30:25|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|          null|        false|{numFiles -> 5, n...|        null|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleTable.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221e6b1-f27e-474f-947e-82e83de21688",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "Important to know this is a delta feature and not a spark feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "69bb6a34-201c-40a4-946c-720be412114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-30 18:30:26\").load(\"s3a://delta/people-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "16bf5e93-e607-43a8-8954-6b66e3602254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"s3a://delta/people-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "14c2eb93-cfd1-479a-ab28-48f4649cab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   179|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"timestampAfter\", \"2022-12-30 18:30:26\").load(\"s3a://delta/people-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0247347-b2bc-42b2-a2a9-9f3d509c3391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION: 13\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   179|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 12\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 31|  1| Johnz|   178|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 11\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1| Johnz|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 10\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 31|  1| Johns|   178|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 9\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 31|  1| Johns|   178|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 8\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 7\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 6\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 5\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 31|  1|  John|   178|\n",
      "| 56|  6|   His|   156|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 4\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 3\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 2\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 1\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   177|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n",
      "VERSION: 0\n",
      "+---+---+------+------+\n",
      "|age| id|  name|weight|\n",
      "+---+---+------+------+\n",
      "| 27|  4|Heimer|   201|\n",
      "| 22|  3|Jingle|   188|\n",
      "| 35|  2| Jacob|   166|\n",
      "| 30|  1|  John|   175|\n",
      "| 38|  5| Smith|  null|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = peopleTable.history()\n",
    "\n",
    "for row in history.collect():\n",
    "    ver = row.version\n",
    "    df = spark.read.format(\"delta\").option(\"versionAsOf\", ver).load(\"s3a://delta/people-table\")\n",
    "    print(f\"VERSION: {ver}\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a29d4-9507-4175-aad5-8c0870d9c344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
