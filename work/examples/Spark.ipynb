{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376497f0-2186-480f-84f4-5e7f7707dbdf",
   "metadata": {},
   "source": [
    "# Spark Basic Environment Information\n",
    "\n",
    "This is useful if you want to install maven dependencies Knowing the spark version (3.1.2) and Scala version (2.12) are important!\n",
    "\n",
    "Dependencies can be included using the following configuration to the session builder. For more than one config, add another line.\n",
    "\n",
    "`    .config(\"spark.jars.packages\",\"groupId:artifactId:version\")\\`\n",
    "\n",
    "For example for this maven config. https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.12/3.1.2\n",
    "\n",
    "the setup would be:\n",
    "\n",
    "`    .config(\"spark.jars.packages\",\"org.apache.spark:spark-hive_2.12:3.1.2\")\\`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6ec98d-1d75-4f77-a899-144ce7e3e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a6e22e-429d-47e2-ba58-4f336d0336a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/05/03 16:49:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Spark init\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff89a7f1-17a3-4051-82c8-1a430913bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context :  <SparkContext master=local appName=jupyter-pyspark>\n",
      "Spark Version :  3.1.2\n",
      "Spark appName : jupyter-pyspark\n",
      "Hadoop version:  3.2.0\n",
      "Spark Confiuration:\n",
      "\tspark.master = local\n",
      "\tspark.driver.host = jupyter\n",
      "\tspark.app.startTime = 1637098089593\n",
      "\tspark.app.id = local-1637098090137\n",
      "\tspark.executor.id = driver\n",
      "\tspark.driver.extraJavaOptions = -Dio.netty.tryReflectionSetAccessible=true\n",
      "\tspark.driver.port = 44993\n",
      "\tspark.app.name = jupyter-pyspark\n",
      "\tspark.rdd.compress = True\n",
      "\tspark.serializer.objectStreamReset = 100\n",
      "\tspark.submit.pyFiles = \n",
      "\tspark.submit.deployMode = client\n",
      "\tspark.executor.extraJavaOptions = -Dio.netty.tryReflectionSetAccessible=true\n",
      "\tspark.sql.warehouse.dir = file:/home/jovyan/work/examples/spark-warehouse\n",
      "\tspark.ui.showConsoleProgress = true\n"
     ]
    }
   ],
   "source": [
    "print('Spark Context : ', spark.sparkContext)\n",
    "print('Spark Version : ', spark.sparkContext.version)\n",
    "print('Spark appName :', spark.sparkContext.appName)\n",
    "print('Hadoop version: ', spark.sparkContext._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n",
    "print('Spark Confiuration:')\n",
    "for conf in spark.sparkContext._conf.getAll():\n",
    "    print(f\"\\t{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c57713-1d16-4031-affb-04d1d59a805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "            com.couchbase.client:spark-connector_2.12:3.1.0,\n",
    "            org.apache.hbase.connectors.spark:hbase-spark:1.0.0\")\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dfeead-c71c-4a6e-979b-ed78af9e71d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.arangodb#arangodb-spark-datasource-3.1_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e6a333de-969b-4762-9fb7-330d4633b4f2;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.arangodb#arangodb-spark-datasource-3.1_2.12;1.4.3 in central\n",
      "\tfound com.arangodb#arangodb-spark-commons-3.1_2.12;1.4.3 in central\n",
      "\tfound com.arangodb#arangodb-java-driver;6.17.0 in central\n",
      "\tfound com.arangodb#velocypack;2.5.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.arangodb#jackson-dataformat-velocypack;3.0.1 in central\n",
      "downloading https://repo1.maven.org/maven2/com/arangodb/arangodb-spark-datasource-3.1_2.12/1.4.3/arangodb-spark-datasource-3.1_2.12-1.4.3.jar ...\n",
      "\t[SUCCESSFUL ] com.arangodb#arangodb-spark-datasource-3.1_2.12;1.4.3!arangodb-spark-datasource-3.1_2.12.jar (114ms)\n",
      "downloading https://repo1.maven.org/maven2/com/arangodb/arangodb-spark-commons-3.1_2.12/1.4.3/arangodb-spark-commons-3.1_2.12-1.4.3.jar ...\n",
      "\t[SUCCESSFUL ] com.arangodb#arangodb-spark-commons-3.1_2.12;1.4.3!arangodb-spark-commons-3.1_2.12.jar (79ms)\n",
      "downloading https://repo1.maven.org/maven2/com/arangodb/arangodb-java-driver/6.17.0/arangodb-java-driver-6.17.0.jar ...\n",
      "\t[SUCCESSFUL ] com.arangodb#arangodb-java-driver;6.17.0!arangodb-java-driver.jar (244ms)\n",
      "downloading https://repo1.maven.org/maven2/com/arangodb/jackson-dataformat-velocypack/3.0.1/jackson-dataformat-velocypack-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.arangodb#jackson-dataformat-velocypack;3.0.1!jackson-dataformat-velocypack.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/com/arangodb/velocypack/2.5.4/velocypack-2.5.4.jar ...\n",
      "\t[SUCCESSFUL ] com.arangodb#velocypack;2.5.4!velocypack.jar (276ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.36!slf4j-api.jar (25ms)\n",
      ":: resolution report :: resolve 3158ms :: artifacts dl 774ms\n",
      "\t:: modules in use:\n",
      "\tcom.arangodb#arangodb-java-driver;6.17.0 from central in [default]\n",
      "\tcom.arangodb#arangodb-spark-commons-3.1_2.12;1.4.3 from central in [default]\n",
      "\tcom.arangodb#arangodb-spark-datasource-3.1_2.12;1.4.3 from central in [default]\n",
      "\tcom.arangodb#jackson-dataformat-velocypack;3.0.1 from central in [default]\n",
      "\tcom.arangodb#velocypack;2.5.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   6   |   6   |   0   ||   6   |   6   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e6a333de-969b-4762-9fb7-330d4633b4f2\n",
      "\tconfs: [default]\n",
      "\t6 artifacts copied, 0 already retrieved (1414kB/10ms)\n",
      "23/05/03 19:06:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "    .config(\"spark.jars.packages\",\"com.arangodb:arangodb-spark-datasource-3.1_2.12:1.4.3\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6a585-a85b-48e1-bfb6-a0fcf992c0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
