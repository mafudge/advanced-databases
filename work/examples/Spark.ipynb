{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376497f0-2186-480f-84f4-5e7f7707dbdf",
   "metadata": {},
   "source": [
    "# Spark Basic Environment Information\n",
    "\n",
    "This is useful if you want to install maven dependencies Knowing the spark version (3.1.2) and Scala version (2.12) are important!\n",
    "\n",
    "Dependencies can be included using the following configuration to the session builder. For more than one config, add another line.\n",
    "\n",
    "`    .config(\"spark.jars.packages\",\"groupId:artifactId:version\")\\`\n",
    "\n",
    "For example for this maven config. https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.12/3.1.2\n",
    "\n",
    "the setup would be:\n",
    "\n",
    "`    .config(\"spark.jars.packages\",\"org.apache.spark:spark-hive_2.12:3.1.2\")\\`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6ec98d-1d75-4f77-a899-144ce7e3e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a6e22e-429d-47e2-ba58-4f336d0336a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark init\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff89a7f1-17a3-4051-82c8-1a430913bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context :  <SparkContext master=local appName=jupyter-pyspark>\n",
      "Spark Version :  3.5.3\n",
      "Spark appName : jupyter-pyspark\n",
      "Hadoop version:  3.3.4\n",
      "Spark Confiuration:\n",
      "\tspark.master = local\n",
      "\tspark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "\tspark.driver.host = jupyter\n",
      "\tspark.driver.port = 43047\n",
      "\tspark.executor.id = driver\n",
      "\tspark.app.submitTime = 1734400324726\n",
      "\tspark.app.name = jupyter-pyspark\n",
      "\tspark.rdd.compress = True\n",
      "\tspark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "\tspark.serializer.objectStreamReset = 100\n",
      "\tspark.submit.pyFiles = \n",
      "\tspark.submit.deployMode = client\n",
      "\tspark.app.id = local-1734400325749\n",
      "\tspark.app.startTime = 1734400324953\n",
      "\tspark.ui.showConsoleProgress = true\n"
     ]
    }
   ],
   "source": [
    "print('Spark Context : ', spark.sparkContext)\n",
    "print('Spark Version : ', spark.sparkContext.version)\n",
    "print('Spark appName :', spark.sparkContext.appName)\n",
    "print('Hadoop version: ', spark.sparkContext._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n",
    "print('Spark Confiuration:')\n",
    "for conf in spark.sparkContext._conf.getAll():\n",
    "    print(f\"\\t{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c57713-1d16-4031-affb-04d1d59a805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66dfeead-c71c-4a6e-979b-ed78af9e71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of pulling in the Arrango Db Spark datasource\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "    .config(\"spark.jars.packages\",\"com.arangodb:arangodb-spark-datasource-3.1_2.12:1.4.3\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6a585-a85b-48e1-bfb6-a0fcf992c0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
